{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API test multi-step test case generation\n",
    "\n",
    "<br>\n",
    "\n",
    "**Schema:**\n",
    "\n",
    "<br>\n",
    "\n",
    "```txt\n",
    "[User/Scenario Modeling] --> [Case Type Generator] --> [Data Expansion]  \n",
    "                                   |                        |  \n",
    "                                   v                        v  \n",
    "                        [Context-Aware Generator] --> [Synthetic Data Output]  \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pprint import pprint\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "agent = Agent(\"openai:gpt-4o-mini\", name=\"test_case_generator\", retries=1)\n",
    "\n",
    "\n",
    "@agent.system_prompt\n",
    "def test_case_system_prompt():\n",
    "    return \"\"\"You are a helpful test case generator. You will be given a description of functionality \n",
    "    and should generate test cases that thoroughly validate the described behavior.\n",
    "    \n",
    "    <TEST CASE OUTPUT FORMAT>\n",
    "    name: <name of the test case>\n",
    "    description: <description of the test case>\n",
    "    input: <input values for the test case>\n",
    "    expected_output: <expected output/behavior of the test case>\n",
    "    preconditions: <any relevant preconditions for the test case>\n",
    "    </TEST CASE OUTPUT FORMAT>\n",
    "\n",
    "    <OUTPUT FORMAT: list of dictionaries>\n",
    "    [<TEST CASE OUTPUT FORMAT>, <TEST CASE OUTPUT FORMAT>, ...]\n",
    "    </OUTPUT FORMAT>\n",
    "\n",
    "    Be aware of the number of test casaes the user wants and output the correct number of test cases in the correct output format.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "agent.result_validator = lambda x: isinstance(x, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_spec = {\n",
    "    \"paths\": {\n",
    "        \"/pets\": {\n",
    "            \"get\": {\n",
    "                \"parameters\": [\n",
    "                    {\n",
    "                        \"name\": \"status\",\n",
    "                        \"in\": \"query\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"required\": True,\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"post\": {\n",
    "                \"operationId\": \"adoptPet\",\n",
    "                \"parameters\": [\n",
    "                    {\n",
    "                        \"name\": \"petId\",\n",
    "                        \"in\": \"query\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"required\": True,\n",
    "                    }\n",
    "                ],\n",
    "                \"responses\": {\n",
    "                    \"200\": {\n",
    "                        \"description\": \"Pet adoption successful\",\n",
    "                        \"content\": {\n",
    "                            \"application/json\": {\n",
    "                                \"schema\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\"message\": {\"type\": \"string\"}, \"adoptionId\": {\"type\": \"string\"}},\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent.run(\"Generate one case for this API spec: \" + str(api_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"name='Simultaneous Requests for Pet Care Articles' description='Send multiple simultaneous GET requests to the /pets endpoint to retrieve various pet care articles, checking the load time for each response.' input_json=None expected_output_prompt='Each request should return within 2 seconds without any errors.' expected_output_json=None preconditions='API endpoint for /pets is operational and contains articles.'\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': 'Simultaneous Requests for Pet Care Articles', 'description': 'Send multiple simultaneous GET requests to the /pets endpoint to retrieve various pet care articles, checking the load time for each response.', 'input_json': None, 'expected_output_prompt': 'Each request should return within 2 seconds without any errors.', 'expected_output_json': None, 'preconditions': 'API endpoint for /pets is operational and contains articles.'}\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response.data[0].model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'description': 'Test retrieving a list of pets with a valid status',\n",
      "  'expected_output_json': None,\n",
      "  'expected_output_prompt': None,\n",
      "  'input_json': None,\n",
      "  'name': 'GetPetsByStatus',\n",
      "  'preconditions': 'The API server is running and has pets with status '\n",
      "                   \"'available' in the database.\"}]\n"
     ]
    }
   ],
   "source": [
    "class TestCase(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_json: dict[str, Any] | list[dict[str, Any]] | None = None\n",
    "    expected_output_prompt: str | None = None\n",
    "    expected_output_json: dict[str, Any] | list[dict[str, Any]] | None = None\n",
    "    preconditions: str | None = None\n",
    "\n",
    "\n",
    "class SuiteTestCases(RootModel[list[TestCase]]):\n",
    "    pass\n",
    "\n",
    "\n",
    "test_cases = SuiteTestCases.model_validate_json(response.data)\n",
    "\n",
    "pprint(test_cases.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 User Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class UserPersona(BaseModel):\n",
    "    persona_type: str\n",
    "    persona: str\n",
    "    primary_intentions: str\n",
    "    secondary_intentions: str\n",
    "\n",
    "\n",
    "user_modelling_agent = Agent(\n",
    "    \"openai:gpt-4o-mini\", name=\"user_modelling_agent\", retries=1, result_type=list[UserPersona]\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserModellingDeps:\n",
    "    known_users: str\n",
    "\n",
    "\n",
    "@user_modelling_agent.system_prompt\n",
    "def user_modelling_prompt(ctx: RunContext[UserModellingDeps]) -> str:\n",
    "    return f\"\"\"\n",
    "    Role:\n",
    "    You are a strategic analyst tasked with identifying high-level user and service personas that may interact with an API or ML/AI system. Your goal is to surface potential users (both individual and service-level) and their general intentions for engaging with the system. You will work from a mix of known users and inferred personas, expanding the list to ensure diverse representation. The output should guide further development of detailed scenarios and workflows.\n",
    "\n",
    "    Objective:\n",
    "\n",
    "    Identify key user personas (e.g., developers, analysts, operators) and service personas (e.g., monitoring services, data ingestion pipelines).\n",
    "    Capture high-level intentions for each persona, representing their goals and types of interactions.\n",
    "    Expand known user/service types into broader categories to ensure full-spectrum coverage.\n",
    "    Distinguish between direct users (interacting directly with the API) and indirect users/services (operating through automated processes or third-party tools).\n",
    "    Instructions:\n",
    "\n",
    "    Persona Identification:\n",
    "    {\"The following users are known to interact with the system: \" + ctx.deps.known_users if ctx.deps is not None else ''}\n",
    "\n",
    "    Start with known user types or services that interact with the system.\n",
    "    Expand the list by identifying adjacent personas or services that share similar goals or operate in overlapping domains.\n",
    "    Consider both individual personas (e.g., data scientists, IT admins) and automated service personas (e.g., logging pipelines, monitoring tools).\n",
    "    Intent Mapping:\n",
    "\n",
    "    For each persona, identify primary intentions (e.g., data retrieval, model evaluation, anomaly detection).\n",
    "    Include secondary intentions (e.g., performance optimization, adversarial testing) to reflect edge or less common use cases.\n",
    "    Prioritize diverse goals that span operational, analytical, and exploratory use cases.\n",
    "    Abstraction Levels:\n",
    "\n",
    "    Keep intentions broad and conceptual (e.g., \"monitor system health,\" \"fetch analytics data\").\n",
    "    Avoid specific API endpoints or technical steps—focus on overarching objectives.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await user_modelling_agent.run(\n",
    "    \"Generate the user personas for this API spec: \" + str(api_spec),\n",
    "    deps=UserModellingDeps(known_users=\"A young man who wants to adopt a pet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UserPersona(persona_type='Individual User', persona='Pet Adopter', primary_intentions='Explore available pets with specific statuses (e.g., available, adopted)', secondary_intentions='Learn about pet care and adoption processes'),\n",
      " UserPersona(persona_type='Individual User', persona='Pet Owner', primary_intentions='Update pet information such as name and age', secondary_intentions='Retrieve history of adopted pets or update status of pets'),\n",
      " UserPersona(persona_type='Developer', persona='API Integrator', primary_intentions='Integrate pet adoption API into applications for better user experience', secondary_intentions='Test endpoint functionalities for performance and reliability'),\n",
      " UserPersona(persona_type='Data Analyst', persona='Adoption Trend Analyst', primary_intentions='Analyze trends in pet adoptions based on status, age, and time periods', secondary_intentions='Generate reports on adoptions for stakeholders'),\n",
      " UserPersona(persona_type='Service', persona='Monitoring Service', primary_intentions='Monitor API uptime and response times for service availability', secondary_intentions='Alert relevant stakeholders in case of failures or performance issues'),\n",
      " UserPersona(persona_type='Service', persona='Logging Service', primary_intentions='Log API requests and responses for auditing and debugging purposes', secondary_intentions='Analyze logs for error patterns or unusual usage spikes')]\n"
     ]
    }
   ],
   "source": [
    "pprint(response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Test Case Family\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class TestCaseFamily(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    test_case_type: str\n",
    "    test_variations: list[str]\n",
    "\n",
    "\n",
    "test_case_family_agent = Agent(\n",
    "    \"openai:gpt-4o-mini\", name=\"test_case_family_agent\", retries=1, result_type=list[TestCaseFamily]\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestCaseFamilyDeps:\n",
    "    pass\n",
    "\n",
    "\n",
    "@test_case_family_agent.system_prompt\n",
    "def test_case_family_prompt(ctx: RunContext[TestCaseFamilyDeps]) -> str:\n",
    "    return \"\"\"\n",
    "    Role:\n",
    "    You are a test case generation expert, responsible for expanding high-level user and service personas into detailed, diverse test cases. Your objective is to ensure the system is thoroughly validated across all types of scenarios, including normal workflows, edge cases, and stress tests. You anticipate potential system weaknesses by generating test cases that reflect both typical and extreme usage patterns.\n",
    "\n",
    "    Objective:\n",
    "\n",
    "    Generate detailed test cases covering normal, edge, and stress conditions for each persona or service.\n",
    "    Simulate realistic API interactions while ensuring exhaustive coverage of potential failure points.\n",
    "    Classify each test case by case type and expected outcome (e.g., success, failure, error handling).\n",
    "    Instructions:\n",
    "\n",
    "    Input Interpretation:\n",
    "\n",
    "    Take high-level personas and use cases as input (e.g., frontend developer fetching data, automated ETL pipelines).\n",
    "    For each persona or service, consider typical paths and potential deviations that may cause failures or inefficiencies.\n",
    "    Case Generation:\n",
    "\n",
    "    Normal Cases: Generate standard, expected interactions where the API functions as intended.\n",
    "    Edge Cases: Push boundaries by creating scenarios that test minimum/maximum values, invalid input formats, or unusual API sequences.\n",
    "    Stress Cases: Simulate high loads, frequent requests, or massive datasets to test system scalability and reliability.\n",
    "    Parameter Variation:\n",
    "\n",
    "    Generate test cases that vary API parameters, payload sizes, and data types to ensure broad coverage.\n",
    "    Account for dependency relationships between fields (e.g., date fields must follow logical order).\n",
    "    Classification:\n",
    "\n",
    "    Tag each test case with the appropriate category:\n",
    "    Normal – Routine, everyday interactions.\n",
    "    Edge – Boundary conditions or rare inputs.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_personas = response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TestCaseFamily(name='Explore Available Pets by Status - Normal Flow', description='The user searches for pets that are available for adoption using the specified status filter.', test_case_type='Normal', test_variations=['Search for pets available for adoption with filters (e.g., age, breed)', 'Search for pets that have been adopted', 'Search for pets with no filters applied']),\n",
      " TestCaseFamily(name='Explore Available Pets by Status - Edge Case', description='The user searches using extreme filters for pet availability using statuses that may not return any results.', test_case_type='Edge', test_variations=[\"Search for pets with a status that does not exist (e.g., 'lost')\", 'Search for pets with a very narrow breed filter that may return few to no results', 'Search for pets with an invalid age range (e.g., age 0 or -1)']),\n",
      " TestCaseFamily(name='Explore Available Pets by Status - Stress Test', description='The user performs multiple rapid searches for pets to test system response under high load.', test_case_type='Stress', test_variations=['Simultaneously search for pets available for adoption across different statuses (e.g., available, adopted) every 1 second for 10 minutes', 'Filter pets by various parameters continuously for an extended period', 'Conduct bulk searches by status with different combinations of filters (age, breed)']),\n",
      " TestCaseFamily(name='Learn About Pet Care - Normal Flow', description='The user accesses the pet care and adoption resources available on the platform.', test_case_type='Normal', test_variations=['View articles on pet care and tips for new adopters', 'Access resources on the adoption process and requirements', 'Open FAQs regarding pet care and adoption procedures']),\n",
      " TestCaseFamily(name='Learn About Pet Care - Edge Case', description='The user tries to access pet care resources using unusable input or edge conditions.', test_case_type='Edge', test_variations=['Attempt to search for pet care resources using an empty search term', 'Navigate to outdated or invalid resource links', 'Request information for a pet type that is not covered (e.g., exotic pets)']),\n",
      " TestCaseFamily(name='Learn About Pet Care - Stress Test', description='The user attempts to access pet care resources during high traffic periods, simulating peak usage.', test_case_type='Stress', test_variations=['Simultaneously request multiple pet care articles and resources to test page load time', 'Access resources repeatedly in quick succession to evaluate system durability', 'Perform high-volume searches for pet care tips to stress-test the platform'])]\n",
      "[TestCaseFamily(name='Update Pet Information - Normal Case', description='Update the name and age of a pet using valid input data.', test_case_type='Normal', test_variations=[\"Update with name 'Buddy' and age '5'\", \"Update with name 'Mittens' and age '3'\"]),\n",
      " TestCaseFamily(name='Retrieve Pet Adoption History - Normal Case', description='Retrieve the history of previously adopted pets without any errors.', test_case_type='Normal', test_variations=['Fetch adoption history for user with 2 adopted pets', 'Fetch adoption history for user with no adopted pets']),\n",
      " TestCaseFamily(name='Update Pet Status - Normal Case', description=\"Update the status of a pet to 'Adopted' successfully.\", test_case_type='Normal', test_variations=[\"Update status of pet 'Bella' to 'Adopted'\", \"Update status of pet 'Max' to 'In Foster Care'\"]),\n",
      " TestCaseFamily(name='Update Pet Information - Edge Case', description='Attempt to update pet information with boundary values, such as negative age or overly long name.', test_case_type='Edge', test_variations=[\"Update with name 'A' and age '-1'\", 'Update with name exceeding 50 characters']),\n",
      " TestCaseFamily(name='Retrieve Pet Adoption History - Edge Case', description='Fetch adoption history for invalid user IDs or nonexistent pets.', test_case_type='Edge', test_variations=['Fetch history for nonexistent user ID', 'Fetch adoption history for a pet that was never adopted']),\n",
      " TestCaseFamily(name='Update Pet Status - Edge Case', description='Update the status of non-existent pets or use invalid status values.', test_case_type='Edge', test_variations=['Update status of a pet that does not exist', \"Update a valid pet with an invalid status value like 'In Limbo'\"]),\n",
      " TestCaseFamily(name='Update Pet Information - Stress Case', description='Perform multiple updates to pet information in a short time frame to test system limits.', test_case_type='Stress', test_variations=['Update pet information 100 times in quick succession', 'Simultaneously update information for 10 different pets']),\n",
      " TestCaseFamily(name='Retrieve Pet Adoption History - Stress Case', description='Simulate high load by retrieving adoption history for multiple users simultaneously.', test_case_type='Stress', test_variations=['Fetch adoption history for 100 different users at once', 'Retrieve 1000 records of adoption history in one request']),\n",
      " TestCaseFamily(name='Update Pet Status - Stress Case', description='Update pet statuses for a large number of pets at once to test API limits.', test_case_type='Stress', test_variations=['Batch update statuses for 100 pets', 'Simultaneous status updates for 50 pets at a time'])]\n",
      "[TestCaseFamily(name='Basic Integration Test', description='Verify successful integration by making a GET request to fetch available pets for adoption.', test_case_type='Normal', test_variations=['GET /api/pets/available', 'Expected response contains list of available pets', 'Response time is under 2 seconds']),\n",
      " TestCaseFamily(name='Invalid Endpoint Test', description='Attempt to access an invalid endpoint to ensure proper error handling.', test_case_type='Edge', test_variations=['GET /api/pets/notfound', 'Expected response: 404 Not Found', 'Response time is under 1 second']),\n",
      " TestCaseFamily(name='Empty Query Parameter Test', description='Test retrieving pets with empty query parameters to check system behavior.', test_case_type='Edge', test_variations=['GET /api/pets?type=', 'Expected response: 400 Bad Request', 'Response message indicates missing parameters']),\n",
      " TestCaseFamily(name='Large Dataset Retrieval Test', description=\"Test the API's ability to handle requests for a large dataset by fetching multiple pages of pets.\", test_case_type='Stress', test_variations=['GET /api/pets?page=100', 'Expected response returns last page of pets', 'Total pets fetched is as expected with pagination']),\n",
      " TestCaseFamily(name='Concurrent Requests Test', description='Simulate multiple concurrent requests to test API performance under load.', test_case_type='Stress', test_variations=['Send 100 GET requests for available pets simultaneously', 'Expected response time for each request is under 1 second', 'Check for any timeouts or errors']),\n",
      " TestCaseFamily(name='Malformed JSON Test', description=\"Send malformed JSON in a POST request to test API's validation handling.\", test_case_type='Edge', test_variations=['POST /api/pets/adopt with invalid JSON structure', 'Expected response: 400 Bad Request', 'Response contains details about JSON formatting error']),\n",
      " TestCaseFamily(name='Authentication Failure Test', description='Attempt to access protected endpoints without authentication to verify security setup.', test_case_type='Edge', test_variations=['GET /api/pets/protected without token', 'Expected response: 401 Unauthorized', 'Verify that the system does not expose sensitive information']),\n",
      " TestCaseFamily(name='Performance Under Load Test', description='Measure response time and performance when fetching a large number of pets simultaneously.', test_case_type='Stress', test_variations=['GET /api/pets?limit=1000', 'Expected response under 5 seconds', 'Monitor server resource utilization'])]\n",
      "[TestCaseFamily(name='Normal Case 1: Fetch Pet Adoption Trends', description='The data analyst successfully fetches adoption trends based on specified parameters including status, age, and period.', test_case_type='Normal', test_variations=['Parameters: status=adopted, age=puppy, time_period=last_month', 'Parameters: status=adopted, age=senior, time_period=last_year', 'Parameters: status=all, age=kitten, time_period=current_week']),\n",
      " TestCaseFamily(name='Normal Case 2: Generate Adoption Report', description='A report is generated correctly for stakeholders with accurate data on pet adoption trends.', test_case_type='Normal', test_variations=['Time period: last_quarter', 'Status: adopted only', 'Age groups: 0-1 years, 1-5 years, 5+ years']),\n",
      " TestCaseFamily(name='Edge Case 1: Filter by Uncommon Age', description=\"The analyst tries to analyze adoption trends for a rarely adopted age category, such as 'unknown' age.\", test_case_type='Edge', test_variations=['Parameters: status=adopted, age=unknown, time_period=all', 'Parameters: status=adopted, age=oldest_age_category, time_period=current_year']),\n",
      " TestCaseFamily(name='Edge Case 2: Filter by Historical Data', description='Fetch adoption trends for a historical time period that may not have records.', test_case_type='Edge', test_variations=['Parameters: status=all, age=adult, time_period=2020', 'Parameters: status=not adopted, age=senior, time_period=2018']),\n",
      " TestCaseFamily(name='Stress Case 1: High Volume of Requests', description='Simulate the analyst making a high volume of requests for adoption data simultaneously.', test_case_type='Stress', test_variations=['Number of requests: 1000 in 1 minute, status=adopted, age=kitten', 'Number of requests: 500 simultaneous requests with varying periods and statuses']),\n",
      " TestCaseFamily(name='Stress Case 2: Large Dataset Retrieval', description='The analyst requests a large dataset for pet adoptions over an extended time period with multiple filters.', test_case_type='Stress', test_variations=['Parameters: status=all, age=any, time_period=10_years', 'Parameters: status=adopted, age=puppy, time_period=last_5_years with 100,000 records requested'])]\n",
      "[TestCaseFamily(name='Check API Uptime', description='Monitor the API status for uptime over a specified period.', test_case_type='Normal', test_variations=['Check uptime every minute for 24 hours', 'Check uptime every hourly for 7 days']),\n",
      " TestCaseFamily(name='Monitor API Response Times', description='Measure the response times of the API under normal conditions.', test_case_type='Normal', test_variations=['Capture response time for 100 consecutive requests', 'Monitor response time with different request payload sizes']),\n",
      " TestCaseFamily(name='Alert on API Failure', description='Confirm that alerts are sent when the API is down.', test_case_type='Normal', test_variations=['Simulate API downtime and check alert notifications', 'Ensure alerts are sent to multiple stakeholders upon API failure']),\n",
      " TestCaseFamily(name='Monitor High Response Time', description='Check if the system alerts on response times exceeding defined thresholds.', test_case_type='Edge', test_variations=['Simulate response times that take longer than 2 seconds', 'Test response time just above the threshold of acceptable performance']),\n",
      " TestCaseFamily(name='API Response with Invalid Data', description='Test how the monitoring service handles receiving invalid data responses from the API.', test_case_type='Edge', test_variations=['Inject invalid JSON into the API response', 'Simulate a scenario where the API responds with a 500 Internal Server Error']),\n",
      " TestCaseFamily(name='Simulate High Load on API', description='Stress test the monitoring service by simulating a high number of API requests.', test_case_type='Stress', test_variations=['Send 1000 requests per second to the API', 'Simulate a load test over an extended period of 1 hour']),\n",
      " TestCaseFamily(name='Recovery from API Downtime', description='Ensure that the service can recover and monitor the API after it comes back online.', test_case_type='Stress', test_variations=['Check monitoring status after a 10-minute downtime', 'Validate alerts after recovery from downtime'])]\n",
      "[TestCaseFamily(name='Log Request Normal Case', description='Log an API request with valid parameters and receive a success response.', test_case_type='Normal', test_variations=['Valid API request with all necessary headers and body', 'Valid API request hitting a performance threshold']),\n",
      " TestCaseFamily(name='Log Response Normal Case', description='Log an API response with valid parameters and ensure it is correctly stored.', test_case_type='Normal', test_variations=['Valid API response with all expected fields', 'Valid API response with a subset of fields']),\n",
      " TestCaseFamily(name='Log Request Edge Case - Invalid JSON', description='Send an API request with invalid JSON format to test error logging.', test_case_type='Edge', test_variations=['Malformed JSON in the request body', 'Empty JSON object in the request body']),\n",
      " TestCaseFamily(name='Log Response Edge Case - Empty Body', description='Receive an API response with an empty body and validate logging behavior.', test_case_type='Edge', test_variations=['API responds with a status code but empty body', 'API responds with error status code and empty body']),\n",
      " TestCaseFamily(name='Log Request Edge Case - Missing Headers', description='Try logging an API request missing required headers.', test_case_type='Edge', test_variations=['API request without authentication token header', 'API request without content type header']),\n",
      " TestCaseFamily(name='Log Request Stress Case - High Throughput', description='Simulate a high volume of API requests to test logging under stress.', test_case_type='Stress', test_variations=['1500 requests per minute', '5000 requests spread over 10 minutes']),\n",
      " TestCaseFamily(name='Log Response Stress Case - Large Payloads', description='Log responses with large payloads to test system limits and performance.', test_case_type='Stress', test_variations=['Responses with payload size of 10MB', 'Responses with payload size of 50MB']),\n",
      " TestCaseFamily(name='Analyze Logs for Error Patterns', description='Perform log analysis after introducing various error scenarios and monitor for detection accuracy.', test_case_type='Normal', test_variations=['Log entries with different error codes', 'Log entries with timestamps indicating unusual spikes']),\n",
      " TestCaseFamily(name='Analyze Logs Edge Case - Corrupted Log File', description='Test system response to a corrupted log file during analysis.', test_case_type='Edge', test_variations=['Log file missing entries', 'Log file with invalid format']),\n",
      " TestCaseFamily(name='Analyze Logs Stress Case - Massive Log Data', description='Simulate analysis of a massive dataset of logs to test performance and efficiency.', test_case_type='Stress', test_variations=['Analyze log entries over a year', 'Analyze logs with 1 million+ entries'])]\n"
     ]
    }
   ],
   "source": [
    "out_test_case_families_per_persona: dict[UserPersona, list[TestCaseFamily]] = dict()\n",
    "\n",
    "for user_persona in out_personas:\n",
    "    response = await test_case_family_agent.run(\n",
    "        \"Generate the test case families for this user persona: \" + str(user_persona),\n",
    "        deps=TestCaseFamilyDeps(known_users=user_persona),\n",
    "    )\n",
    "    out_test_case_families_per_persona[str(user_persona)] = response.data\n",
    "    pprint(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"persona_type='Individual User' persona='Pet Adopter' primary_intentions='Explore available pets with specific statuses (e.g., available, adopted)' secondary_intentions='Learn about pet care and adoption processes'\": [TestCaseFamily(name='Explore Available Pets by Status - Normal Flow', description='The user searches for pets that are available for adoption using the specified status filter.', test_case_type='Normal', test_variations=['Search for pets available for adoption with filters (e.g., age, breed)', 'Search for pets that have been adopted', 'Search for pets with no filters applied']),\n",
       "  TestCaseFamily(name='Explore Available Pets by Status - Edge Case', description='The user searches using extreme filters for pet availability using statuses that may not return any results.', test_case_type='Edge', test_variations=[\"Search for pets with a status that does not exist (e.g., 'lost')\", 'Search for pets with a very narrow breed filter that may return few to no results', 'Search for pets with an invalid age range (e.g., age 0 or -1)']),\n",
       "  TestCaseFamily(name='Explore Available Pets by Status - Stress Test', description='The user performs multiple rapid searches for pets to test system response under high load.', test_case_type='Stress', test_variations=['Simultaneously search for pets available for adoption across different statuses (e.g., available, adopted) every 1 second for 10 minutes', 'Filter pets by various parameters continuously for an extended period', 'Conduct bulk searches by status with different combinations of filters (age, breed)']),\n",
       "  TestCaseFamily(name='Learn About Pet Care - Normal Flow', description='The user accesses the pet care and adoption resources available on the platform.', test_case_type='Normal', test_variations=['View articles on pet care and tips for new adopters', 'Access resources on the adoption process and requirements', 'Open FAQs regarding pet care and adoption procedures']),\n",
       "  TestCaseFamily(name='Learn About Pet Care - Edge Case', description='The user tries to access pet care resources using unusable input or edge conditions.', test_case_type='Edge', test_variations=['Attempt to search for pet care resources using an empty search term', 'Navigate to outdated or invalid resource links', 'Request information for a pet type that is not covered (e.g., exotic pets)']),\n",
       "  TestCaseFamily(name='Learn About Pet Care - Stress Test', description='The user attempts to access pet care resources during high traffic periods, simulating peak usage.', test_case_type='Stress', test_variations=['Simultaneously request multiple pet care articles and resources to test page load time', 'Access resources repeatedly in quick succession to evaluate system durability', 'Perform high-volume searches for pet care tips to stress-test the platform'])],\n",
       " \"persona_type='Individual User' persona='Pet Owner' primary_intentions='Update pet information such as name and age' secondary_intentions='Retrieve history of adopted pets or update status of pets'\": [TestCaseFamily(name='Update Pet Information - Normal Case', description='Update the name and age of a pet using valid input data.', test_case_type='Normal', test_variations=[\"Update with name 'Buddy' and age '5'\", \"Update with name 'Mittens' and age '3'\"]),\n",
       "  TestCaseFamily(name='Retrieve Pet Adoption History - Normal Case', description='Retrieve the history of previously adopted pets without any errors.', test_case_type='Normal', test_variations=['Fetch adoption history for user with 2 adopted pets', 'Fetch adoption history for user with no adopted pets']),\n",
       "  TestCaseFamily(name='Update Pet Status - Normal Case', description=\"Update the status of a pet to 'Adopted' successfully.\", test_case_type='Normal', test_variations=[\"Update status of pet 'Bella' to 'Adopted'\", \"Update status of pet 'Max' to 'In Foster Care'\"]),\n",
       "  TestCaseFamily(name='Update Pet Information - Edge Case', description='Attempt to update pet information with boundary values, such as negative age or overly long name.', test_case_type='Edge', test_variations=[\"Update with name 'A' and age '-1'\", 'Update with name exceeding 50 characters']),\n",
       "  TestCaseFamily(name='Retrieve Pet Adoption History - Edge Case', description='Fetch adoption history for invalid user IDs or nonexistent pets.', test_case_type='Edge', test_variations=['Fetch history for nonexistent user ID', 'Fetch adoption history for a pet that was never adopted']),\n",
       "  TestCaseFamily(name='Update Pet Status - Edge Case', description='Update the status of non-existent pets or use invalid status values.', test_case_type='Edge', test_variations=['Update status of a pet that does not exist', \"Update a valid pet with an invalid status value like 'In Limbo'\"]),\n",
       "  TestCaseFamily(name='Update Pet Information - Stress Case', description='Perform multiple updates to pet information in a short time frame to test system limits.', test_case_type='Stress', test_variations=['Update pet information 100 times in quick succession', 'Simultaneously update information for 10 different pets']),\n",
       "  TestCaseFamily(name='Retrieve Pet Adoption History - Stress Case', description='Simulate high load by retrieving adoption history for multiple users simultaneously.', test_case_type='Stress', test_variations=['Fetch adoption history for 100 different users at once', 'Retrieve 1000 records of adoption history in one request']),\n",
       "  TestCaseFamily(name='Update Pet Status - Stress Case', description='Update pet statuses for a large number of pets at once to test API limits.', test_case_type='Stress', test_variations=['Batch update statuses for 100 pets', 'Simultaneous status updates for 50 pets at a time'])],\n",
       " \"persona_type='Developer' persona='API Integrator' primary_intentions='Integrate pet adoption API into applications for better user experience' secondary_intentions='Test endpoint functionalities for performance and reliability'\": [TestCaseFamily(name='Basic Integration Test', description='Verify successful integration by making a GET request to fetch available pets for adoption.', test_case_type='Normal', test_variations=['GET /api/pets/available', 'Expected response contains list of available pets', 'Response time is under 2 seconds']),\n",
       "  TestCaseFamily(name='Invalid Endpoint Test', description='Attempt to access an invalid endpoint to ensure proper error handling.', test_case_type='Edge', test_variations=['GET /api/pets/notfound', 'Expected response: 404 Not Found', 'Response time is under 1 second']),\n",
       "  TestCaseFamily(name='Empty Query Parameter Test', description='Test retrieving pets with empty query parameters to check system behavior.', test_case_type='Edge', test_variations=['GET /api/pets?type=', 'Expected response: 400 Bad Request', 'Response message indicates missing parameters']),\n",
       "  TestCaseFamily(name='Large Dataset Retrieval Test', description=\"Test the API's ability to handle requests for a large dataset by fetching multiple pages of pets.\", test_case_type='Stress', test_variations=['GET /api/pets?page=100', 'Expected response returns last page of pets', 'Total pets fetched is as expected with pagination']),\n",
       "  TestCaseFamily(name='Concurrent Requests Test', description='Simulate multiple concurrent requests to test API performance under load.', test_case_type='Stress', test_variations=['Send 100 GET requests for available pets simultaneously', 'Expected response time for each request is under 1 second', 'Check for any timeouts or errors']),\n",
       "  TestCaseFamily(name='Malformed JSON Test', description=\"Send malformed JSON in a POST request to test API's validation handling.\", test_case_type='Edge', test_variations=['POST /api/pets/adopt with invalid JSON structure', 'Expected response: 400 Bad Request', 'Response contains details about JSON formatting error']),\n",
       "  TestCaseFamily(name='Authentication Failure Test', description='Attempt to access protected endpoints without authentication to verify security setup.', test_case_type='Edge', test_variations=['GET /api/pets/protected without token', 'Expected response: 401 Unauthorized', 'Verify that the system does not expose sensitive information']),\n",
       "  TestCaseFamily(name='Performance Under Load Test', description='Measure response time and performance when fetching a large number of pets simultaneously.', test_case_type='Stress', test_variations=['GET /api/pets?limit=1000', 'Expected response under 5 seconds', 'Monitor server resource utilization'])],\n",
       " \"persona_type='Data Analyst' persona='Adoption Trend Analyst' primary_intentions='Analyze trends in pet adoptions based on status, age, and time periods' secondary_intentions='Generate reports on adoptions for stakeholders'\": [TestCaseFamily(name='Normal Case 1: Fetch Pet Adoption Trends', description='The data analyst successfully fetches adoption trends based on specified parameters including status, age, and period.', test_case_type='Normal', test_variations=['Parameters: status=adopted, age=puppy, time_period=last_month', 'Parameters: status=adopted, age=senior, time_period=last_year', 'Parameters: status=all, age=kitten, time_period=current_week']),\n",
       "  TestCaseFamily(name='Normal Case 2: Generate Adoption Report', description='A report is generated correctly for stakeholders with accurate data on pet adoption trends.', test_case_type='Normal', test_variations=['Time period: last_quarter', 'Status: adopted only', 'Age groups: 0-1 years, 1-5 years, 5+ years']),\n",
       "  TestCaseFamily(name='Edge Case 1: Filter by Uncommon Age', description=\"The analyst tries to analyze adoption trends for a rarely adopted age category, such as 'unknown' age.\", test_case_type='Edge', test_variations=['Parameters: status=adopted, age=unknown, time_period=all', 'Parameters: status=adopted, age=oldest_age_category, time_period=current_year']),\n",
       "  TestCaseFamily(name='Edge Case 2: Filter by Historical Data', description='Fetch adoption trends for a historical time period that may not have records.', test_case_type='Edge', test_variations=['Parameters: status=all, age=adult, time_period=2020', 'Parameters: status=not adopted, age=senior, time_period=2018']),\n",
       "  TestCaseFamily(name='Stress Case 1: High Volume of Requests', description='Simulate the analyst making a high volume of requests for adoption data simultaneously.', test_case_type='Stress', test_variations=['Number of requests: 1000 in 1 minute, status=adopted, age=kitten', 'Number of requests: 500 simultaneous requests with varying periods and statuses']),\n",
       "  TestCaseFamily(name='Stress Case 2: Large Dataset Retrieval', description='The analyst requests a large dataset for pet adoptions over an extended time period with multiple filters.', test_case_type='Stress', test_variations=['Parameters: status=all, age=any, time_period=10_years', 'Parameters: status=adopted, age=puppy, time_period=last_5_years with 100,000 records requested'])],\n",
       " \"persona_type='Service' persona='Monitoring Service' primary_intentions='Monitor API uptime and response times for service availability' secondary_intentions='Alert relevant stakeholders in case of failures or performance issues'\": [TestCaseFamily(name='Check API Uptime', description='Monitor the API status for uptime over a specified period.', test_case_type='Normal', test_variations=['Check uptime every minute for 24 hours', 'Check uptime every hourly for 7 days']),\n",
       "  TestCaseFamily(name='Monitor API Response Times', description='Measure the response times of the API under normal conditions.', test_case_type='Normal', test_variations=['Capture response time for 100 consecutive requests', 'Monitor response time with different request payload sizes']),\n",
       "  TestCaseFamily(name='Alert on API Failure', description='Confirm that alerts are sent when the API is down.', test_case_type='Normal', test_variations=['Simulate API downtime and check alert notifications', 'Ensure alerts are sent to multiple stakeholders upon API failure']),\n",
       "  TestCaseFamily(name='Monitor High Response Time', description='Check if the system alerts on response times exceeding defined thresholds.', test_case_type='Edge', test_variations=['Simulate response times that take longer than 2 seconds', 'Test response time just above the threshold of acceptable performance']),\n",
       "  TestCaseFamily(name='API Response with Invalid Data', description='Test how the monitoring service handles receiving invalid data responses from the API.', test_case_type='Edge', test_variations=['Inject invalid JSON into the API response', 'Simulate a scenario where the API responds with a 500 Internal Server Error']),\n",
       "  TestCaseFamily(name='Simulate High Load on API', description='Stress test the monitoring service by simulating a high number of API requests.', test_case_type='Stress', test_variations=['Send 1000 requests per second to the API', 'Simulate a load test over an extended period of 1 hour']),\n",
       "  TestCaseFamily(name='Recovery from API Downtime', description='Ensure that the service can recover and monitor the API after it comes back online.', test_case_type='Stress', test_variations=['Check monitoring status after a 10-minute downtime', 'Validate alerts after recovery from downtime'])],\n",
       " \"persona_type='Service' persona='Logging Service' primary_intentions='Log API requests and responses for auditing and debugging purposes' secondary_intentions='Analyze logs for error patterns or unusual usage spikes'\": [TestCaseFamily(name='Log Request Normal Case', description='Log an API request with valid parameters and receive a success response.', test_case_type='Normal', test_variations=['Valid API request with all necessary headers and body', 'Valid API request hitting a performance threshold']),\n",
       "  TestCaseFamily(name='Log Response Normal Case', description='Log an API response with valid parameters and ensure it is correctly stored.', test_case_type='Normal', test_variations=['Valid API response with all expected fields', 'Valid API response with a subset of fields']),\n",
       "  TestCaseFamily(name='Log Request Edge Case - Invalid JSON', description='Send an API request with invalid JSON format to test error logging.', test_case_type='Edge', test_variations=['Malformed JSON in the request body', 'Empty JSON object in the request body']),\n",
       "  TestCaseFamily(name='Log Response Edge Case - Empty Body', description='Receive an API response with an empty body and validate logging behavior.', test_case_type='Edge', test_variations=['API responds with a status code but empty body', 'API responds with error status code and empty body']),\n",
       "  TestCaseFamily(name='Log Request Edge Case - Missing Headers', description='Try logging an API request missing required headers.', test_case_type='Edge', test_variations=['API request without authentication token header', 'API request without content type header']),\n",
       "  TestCaseFamily(name='Log Request Stress Case - High Throughput', description='Simulate a high volume of API requests to test logging under stress.', test_case_type='Stress', test_variations=['1500 requests per minute', '5000 requests spread over 10 minutes']),\n",
       "  TestCaseFamily(name='Log Response Stress Case - Large Payloads', description='Log responses with large payloads to test system limits and performance.', test_case_type='Stress', test_variations=['Responses with payload size of 10MB', 'Responses with payload size of 50MB']),\n",
       "  TestCaseFamily(name='Analyze Logs for Error Patterns', description='Perform log analysis after introducing various error scenarios and monitor for detection accuracy.', test_case_type='Normal', test_variations=['Log entries with different error codes', 'Log entries with timestamps indicating unusual spikes']),\n",
       "  TestCaseFamily(name='Analyze Logs Edge Case - Corrupted Log File', description='Test system response to a corrupted log file during analysis.', test_case_type='Edge', test_variations=['Log file missing entries', 'Log file with invalid format']),\n",
       "  TestCaseFamily(name='Analyze Logs Stress Case - Massive Log Data', description='Simulate analysis of a massive dataset of logs to test performance and efficiency.', test_case_type='Stress', test_variations=['Analyze log entries over a year', 'Analyze logs with 1 million+ entries'])]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_test_case_families_per_persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Expanded Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(test_cases) for test_cases in out_test_case_families_per_persona.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Field' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTestCase\u001b[39;00m(BaseModel):\n\u001b[0;32m      4\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name of the test case\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     description: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe description of the test case\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[96], line 4\u001b[0m, in \u001b[0;36mTestCase\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTestCase\u001b[39;00m(BaseModel):\n\u001b[1;32m----> 4\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mField\u001b[49m(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name of the test case\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     description: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe description of the test case\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe path of the test case\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Field' is not defined"
     ]
    }
   ],
   "source": [
    "class TestCase(BaseModel):\n",
    "    name: str = Field(description=\"The name of the test case\")\n",
    "    description: str = Field(description=\"The description of the test case\")\n",
    "    path: str = Field(description=\"The path of the test case\")\n",
    "    method: str = Field(description=\"The method of the test case\")\n",
    "    input_json: dict[str, Any] | list[dict[str, Any]] | None = Field(\n",
    "        description=\"The input values for the test case. Should strictly follow the api spec\"\n",
    "    )\n",
    "    expected_output_prompt: str | None = Field(description=\"The expected output/behavior of the test case\")\n",
    "    expected_output_json: dict[str, Any] | list[dict[str, Any]] | None = Field(\n",
    "        description=\"The expected output/behavior of the test case\"\n",
    "    )\n",
    "    preconditions: str | None = Field(description=\"Any relevant preconditions for the test case\")\n",
    "\n",
    "\n",
    "test_case_generator_agent = Agent(\n",
    "    \"openai:gpt-4o-mini\", name=\"test_case_generator_agent\", retries=1, result_type=list[TestCase]\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestCaseGeneratorDeps:\n",
    "    pass\n",
    "\n",
    "\n",
    "@test_case_generator_agent.system_prompt\n",
    "def test_case_generator_prompt(ctx: RunContext[TestCaseGeneratorDeps]) -> str:\n",
    "    return \"\"\"\n",
    "    Role:\n",
    "    You are a data refinement and expansion specialist, responsible for ensuring wide parameter coverage and generating realistic, constraint-aware data for API and ML/AI testing. Your task is to take high-level test cases and enrich them by expanding parameter ranges, exploring edge values, and ensuring the data reflects real-world patterns and constraints.\n",
    "\n",
    "    Objective:\n",
    "\n",
    "    Expand test cases by generating diverse parameter values, ensuring broad coverage of normal, edge, and extreme conditions.\n",
    "    Apply realistic data constraints (e.g., date ranges, field dependencies) to avoid infeasible test scenarios.\n",
    "    Maximize variability across inputs while adhering to domain-specific logic and operational limits.\n",
    "    Instructions:\n",
    "\n",
    "    Parameter Expansion:\n",
    "\n",
    "    For each test case, vary key parameters (e.g., numeric ranges, string lengths, boolean flags).\n",
    "    Generate values across full ranges, including minimum, maximum, and boundary values.\n",
    "    Incorporate random sampling where appropriate to introduce variability.\n",
    "    Constraint Application:\n",
    "\n",
    "    Ensure expanded data respects logical dependencies (e.g., start_date must precede end_date).\n",
    "    Reflect real-world limits (e.g., phone numbers must follow local formats, user IDs must be alphanumeric).\n",
    "    Apply domain-specific constraints (e.g., healthcare data must pass validation rules, financial data must align with regulations).\n",
    "    Data Types and Formats:\n",
    "\n",
    "    Generate diverse formats for fields like dates, strings, and numerical values (e.g., ISO dates, various string encodings).\n",
    "    Vary payload sizes, testing both minimal and maximal inputs.\n",
    "    Edge and Adversarial Data:\n",
    "\n",
    "    Create inputs that test unusual conditions (e.g., empty payloads, long strings, nested JSON structures).\n",
    "    Ensure adversarial data (e.g., special characters, SQL injection patterns) is included to assess API security.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only expand first family\n",
    "\n",
    "out_test_case_families_per_persona[list(out_test_case_families_per_persona.keys())[0]]\n",
    "\n",
    "out_test_cases_per_family: dict[TestCaseFamily, list[TestCase]] = dict()\n",
    "\n",
    "for family in out_test_case_families_per_persona[list(out_test_case_families_per_persona.keys())[0]]:\n",
    "    response = await test_case_generator_agent.run(\n",
    "        f\"Expand the test case family of tests: {family.name}. This possible tests variations are: {family.test_variations}. The api spec is: {api_spec}\",\n",
    "        deps=TestCaseGeneratorDeps(),\n",
    "    )\n",
    "    out_test_cases_per_family[family.name] = response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestCase(name='Search for pets available for adoption with filters (age and breed)', description='Retrieve pets that are available for adoption filtered by specific age and breed criteria.', input_json={'status': 'available', 'age': 2, 'breed': 'Labrador'}, expected_output_prompt='A list of available pets filtered by age and breed.', expected_output_json=None, preconditions='User has access to the pet adoption API and the filtering criteria are valid.')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_test_cases_per_family[list(out_test_cases_per_family.keys())[0]][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_api(api_spec: dict):\n",
    "    \"\"\"Creates a mock API implementation based on the provided OpenAPI specification.\n",
    "\n",
    "    Args:\n",
    "        api_spec (dict): OpenAPI specification defining the API endpoints and schemas\n",
    "\n",
    "    Returns:\n",
    "        dict: Mock API implementation with endpoint handlers\n",
    "    \"\"\"\n",
    "    # Initialize mock data store\n",
    "    mock_data = {\n",
    "        \"pets\": [\n",
    "            {\"id\": \"1\", \"name\": \"Buddy\", \"age\": 3, \"status\": \"available\"},\n",
    "            {\"id\": \"2\", \"name\": \"Max\", \"age\": 2, \"status\": \"available\"},\n",
    "            {\"id\": \"3\", \"name\": \"Luna\", \"age\": 1, \"status\": \"adopted\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def get_pets(status: str = None):\n",
    "        \"\"\"Mock GET /pets endpoint\"\"\"\n",
    "        if status:\n",
    "            return {\"pets\": [pet for pet in mock_data[\"pets\"] if pet[\"status\"] == status]}\n",
    "        return {\"pets\": mock_data[\"pets\"]}\n",
    "\n",
    "    def add_pet(name: str, age: int):\n",
    "        \"\"\"Mock POST /pets endpoint for adding new pets\"\"\"\n",
    "        new_pet = {\"id\": str(len(mock_data[\"pets\"]) + 1), \"name\": name, \"age\": age, \"status\": \"available\"}\n",
    "        mock_data[\"pets\"].append(new_pet)\n",
    "        return new_pet\n",
    "\n",
    "    def adopt_pet(pet_id: str):\n",
    "        \"\"\"Mock POST /pets endpoint for adopting pets\"\"\"\n",
    "        for pet in mock_data[\"pets\"]:\n",
    "            if pet[\"id\"] == pet_id and pet[\"status\"] == \"available\":\n",
    "                pet[\"status\"] = \"adopted\"\n",
    "                return {\"message\": f\"Successfully adopted pet {pet['name']}\", \"adoptionId\": f\"ADOPT-{pet_id}\"}\n",
    "        return {\"error\": \"Pet not found or not available\"}\n",
    "\n",
    "    return {\"endpoints\": {\"GET /pets\": get_pets, \"POST /pets\": add_pet, \"POST /pets/adopt\": adopt_pet}}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "mock_api_instance = mock_api(api_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'The user searches for pets available for adoption filtering '\n",
      "                'by a specific age range (e.g., puppies under 1 year).',\n",
      " 'expected_output_json': None,\n",
      " 'expected_output_prompt': 'List of available pets under 1 year of age.',\n",
      " 'input_json': None,\n",
      " 'name': 'Normal Case - Search for available pets by age filter',\n",
      " 'preconditions': 'User is logged in and has access to the pet search '\n",
      "                  'function.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(out_test_cases_per_family[list(out_test_cases_per_family.keys())[0]][0].model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Agent(model=OpenAIModel(model_name='gpt-4o-mini'), name='test_case_generator', end_strategy='early', model_settings=None),\n",
       " Agent(model=OpenAIModel(model_name='gpt-4o-mini'), name='user_modelling_agent', end_strategy='early', model_settings=None),\n",
       " Agent(model=OpenAIModel(model_name='gpt-4o-mini'), name='test_case_family_agent', end_strategy='early', model_settings=None),\n",
       " Agent(model=OpenAIModel(model_name='gpt-4o-mini'), name='test_case_generator_agent', end_strategy='early', model_settings=None)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AgentInspector:\n",
    "    @staticmethod\n",
    "    def inspect_agent(agent: Agent) -> dict:\n",
    "        \"\"\"Inspects an Agent instance and returns its key attributes and configuration.\n",
    "\n",
    "        Args:\n",
    "            agent: An instance of Agent to inspect\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing agent attributes and configuration\n",
    "        \"\"\"\n",
    "        inspection_result = {\n",
    "            \"name\": agent.name,\n",
    "            \"model\": agent.model,\n",
    "            \"retries\": agent.retries,\n",
    "            \"result_type\": str(agent.result_type),\n",
    "            \"has_system_prompt\": hasattr(agent, \"system_prompt\"),\n",
    "        }\n",
    "        return inspection_result\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_agents() -> list[Agent]:\n",
    "        \"\"\"Returns all Agent instances that have been created.\n",
    "\n",
    "        Returns:\n",
    "            list[Agent]: List of all Agent instances\n",
    "        \"\"\"\n",
    "        return [obj for obj in globals().values() if isinstance(obj, Agent)]\n",
    "\n",
    "\n",
    "AgentInspector.get_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import create_task\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class AgentStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentResult(BaseModel):\n",
    "    status: AgentStatus\n",
    "    data: BaseModel | None = None\n",
    "    msg: str | None = None\n",
    "\n",
    "\n",
    "class AgentOrchestrator:\n",
    "    def __init__(self, agents: list[tuple[Agent, dict[str, Any]]]):\n",
    "        self.agents: list[tuple[Agent, dict[str, Any]]] = agents\n",
    "        self.results: dict[str, dict[str, AgentResult]] = {}\n",
    "\n",
    "    async def execute_agent_with_evaluation(\n",
    "        self,\n",
    "        agent_tuple: tuple[Agent, dict[str, Any]],\n",
    "        **kwargs,\n",
    "    ) -> AgentResult:\n",
    "        agent, agent_kwargs = agent_tuple\n",
    "        print(f\"Executing agent: {agent.name}\")\n",
    "        try:\n",
    "            # Initialize results structure if not exists\n",
    "            if agent.name not in self.results:\n",
    "                self.results[agent.name] = {}\n",
    "                print(f\"Initialized results structure for agent: {agent.name}\")\n",
    "\n",
    "            # Track execution under parent agent if it exists\n",
    "            if \"previous_agent\" in kwargs:\n",
    "                parent_key = f\"{kwargs['previous_agent'].name}_{kwargs.get('task_id', 'default')}\"\n",
    "                self.results[agent.name][parent_key] = AgentResult(status=AgentStatus.RUNNING)\n",
    "                print(f\"Tracking execution under parent agent: {kwargs['previous_agent'].name}\")\n",
    "\n",
    "            # Execute agent\n",
    "            if \"previous_agent\" in kwargs:\n",
    "                user_prompt = agent_kwargs.get(\"user_prompt\", \"\") + f\"{kwargs['previous_result']}\"\n",
    "                print(f\"Running agent with previous result from: {kwargs['previous_agent'].name}\")\n",
    "                result = await agent.run(\n",
    "                    user_prompt=user_prompt,\n",
    "                    **{k: v for k, v in agent_kwargs.items() if k != \"user_prompt\"},\n",
    "                )\n",
    "            else:\n",
    "                print(\"Running agent without previous result\")\n",
    "                result = await agent.run(**agent_kwargs)\n",
    "\n",
    "            # Store result\n",
    "            result_key = f\"{kwargs.get('previous_agent', agent).name}_{kwargs.get('task_id', 'default')}\"\n",
    "            self.results[agent.name][result_key] = AgentResult(\n",
    "                status=AgentStatus.COMPLETED,\n",
    "                data=result.data,\n",
    "            )\n",
    "            print(f\"Stored result for key: {result_key}\")\n",
    "\n",
    "            return self.results[agent.name][result_key]\n",
    "\n",
    "        except Exception as e:\n",
    "            error_key = f\"{kwargs.get('previous_agent', agent).name}_{kwargs.get('task_id', 'default')}\"\n",
    "            self.results[agent.name][error_key] = AgentResult(status=AgentStatus.FAILED, msg=str(e))\n",
    "            print(f\"Error executing agent {agent.name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def run_parallel(self, *args, **kwargs) -> dict[str, AgentResult]:\n",
    "        \"\"\"Execute agents in sequence, but parallelize based on list outputs\"\"\"\n",
    "        print(\"Starting parallel execution of agents\")\n",
    "\n",
    "        async def process_agent_level(\n",
    "            agent_tuple: tuple[Agent, dict[str, Any]],\n",
    "            previous_results: list[tuple[str, Any]] | None = None,\n",
    "            level: int = 0,\n",
    "        ) -> list[AgentResult]:\n",
    "            agent_name = agent_tuple[0].name\n",
    "            print(f\"\\nProcessing agent level {level} with agent: {agent_name}\")\n",
    "            results = []\n",
    "\n",
    "            if previous_results is None:\n",
    "                print(f\"Executing first agent: {agent_name}\")\n",
    "                # First agent - single execution\n",
    "                result = await self.execute_agent_with_evaluation(\n",
    "                    agent_tuple, task_id=f\"level_{level}_task_0\", **kwargs\n",
    "                )\n",
    "                results.append((\"task_0\", result))\n",
    "            else:\n",
    "                print(f\"Processing {len(previous_results)} previous results for agent: {agent_name}\")\n",
    "                # Create tasks for each previous result\n",
    "                tasks = []\n",
    "                for task_id, prev_result in previous_results:\n",
    "                    print(f\"Creating task for previous result: {task_id}\")\n",
    "                    task = create_task(\n",
    "                        self.execute_agent_with_evaluation(\n",
    "                            agent_tuple,\n",
    "                            previous_agent=self.agents[level - 1][0],\n",
    "                            previous_result=prev_result,\n",
    "                            task_id=f\"level_{level}_{task_id}\",\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                    )\n",
    "                    tasks.append((task_id, task))\n",
    "\n",
    "                # Execute all tasks for this level\n",
    "                for task_id, task in tasks:\n",
    "                    try:\n",
    "                        result = await task\n",
    "                        results.append((task_id, result))\n",
    "                        print(f\"Completed task: {task_id}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in task {task_id}: {e}\")\n",
    "\n",
    "            # Prepare results for next level\n",
    "            expanded_results = []\n",
    "            for task_id, result in results:\n",
    "                if result.data:\n",
    "                    data_list = result.data if isinstance(result.data, list) else [result.data]\n",
    "                    print(f\"Expanding {len(data_list)} results from task: {task_id}\")\n",
    "                    for i, data_item in enumerate(data_list):\n",
    "                        expanded_results.append((f\"{task_id}_subtask_{i}\", data_item))\n",
    "\n",
    "            print(f\"Level {level} completed with {len(expanded_results)} expanded results\")\n",
    "            return expanded_results\n",
    "\n",
    "        # Process each level\n",
    "        current_results = None\n",
    "        for level, agent_tuple in enumerate(self.agents):\n",
    "            print(f\"\\nStarting level {level} with agent: {agent_tuple[0].name}\")\n",
    "            current_results = await process_agent_level(agent_tuple, previous_results=current_results, level=level)\n",
    "\n",
    "        print(\"\\nAll levels completed\")\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel execution of agents\n",
      "\n",
      "Starting level 0 with agent: user_modelling_agent\n",
      "\n",
      "Processing agent level 0 with agent: user_modelling_agent\n",
      "Executing first agent: user_modelling_agent\n",
      "Executing agent: user_modelling_agent\n",
      "Initialized results structure for agent: user_modelling_agent\n",
      "Running agent without previous result\n",
      "Stored result for key: user_modelling_agent_level_0_task_0\n",
      "Expanding 6 results from task: task_0\n",
      "Level 0 completed with 6 expanded results\n",
      "\n",
      "Starting level 1 with agent: test_case_family_agent\n",
      "\n",
      "Processing agent level 1 with agent: test_case_family_agent\n",
      "Processing 6 previous results for agent: test_case_family_agent\n",
      "Creating task for previous result: task_0_subtask_0\n",
      "Creating task for previous result: task_0_subtask_1\n",
      "Creating task for previous result: task_0_subtask_2\n",
      "Creating task for previous result: task_0_subtask_3\n",
      "Creating task for previous result: task_0_subtask_4\n",
      "Creating task for previous result: task_0_subtask_5\n",
      "Executing agent: test_case_family_agent\n",
      "Initialized results structure for agent: test_case_family_agent\n",
      "Tracking execution under parent agent: user_modelling_agent\n",
      "Running agent with previous result from: user_modelling_agent\n",
      "Executing agent: test_case_family_agent\n",
      "Tracking execution under parent agent: user_modelling_agent\n",
      "Running agent with previous result from: user_modelling_agent\n",
      "Executing agent: test_case_family_agent\n",
      "Tracking execution under parent agent: user_modelling_agent\n",
      "Running agent with previous result from: user_modelling_agent\n",
      "Executing agent: test_case_family_agent\n",
      "Tracking execution under parent agent: user_modelling_agent\n",
      "Running agent with previous result from: user_modelling_agent\n",
      "Executing agent: test_case_family_agent\n",
      "Tracking execution under parent agent: user_modelling_agent\n",
      "Running agent with previous result from: user_modelling_agent\n",
      "Executing agent: test_case_family_agent\n",
      "Tracking execution under parent agent: user_modelling_agent\n",
      "Running agent with previous result from: user_modelling_agent\n",
      "Stored result for key: user_modelling_agent_level_1_task_0_subtask_1\n",
      "Stored result for key: user_modelling_agent_level_1_task_0_subtask_3\n",
      "Stored result for key: user_modelling_agent_level_1_task_0_subtask_0\n",
      "Completed task: task_0_subtask_0\n",
      "Completed task: task_0_subtask_1\n",
      "Stored result for key: user_modelling_agent_level_1_task_0_subtask_2\n",
      "Completed task: task_0_subtask_2\n",
      "Completed task: task_0_subtask_3\n",
      "Stored result for key: user_modelling_agent_level_1_task_0_subtask_4\n",
      "Completed task: task_0_subtask_4\n",
      "Stored result for key: user_modelling_agent_level_1_task_0_subtask_5\n",
      "Completed task: task_0_subtask_5\n",
      "Expanding 8 results from task: task_0_subtask_0\n",
      "Expanding 5 results from task: task_0_subtask_1\n",
      "Expanding 6 results from task: task_0_subtask_2\n",
      "Expanding 6 results from task: task_0_subtask_3\n",
      "Expanding 8 results from task: task_0_subtask_4\n",
      "Expanding 8 results from task: task_0_subtask_5\n",
      "Level 1 completed with 41 expanded results\n",
      "\n",
      "Starting level 2 with agent: test_case_generator_agent\n",
      "\n",
      "Processing agent level 2 with agent: test_case_generator_agent\n",
      "Processing 41 previous results for agent: test_case_generator_agent\n",
      "Creating task for previous result: task_0_subtask_0_subtask_0\n",
      "Creating task for previous result: task_0_subtask_0_subtask_1\n",
      "Creating task for previous result: task_0_subtask_0_subtask_2\n",
      "Creating task for previous result: task_0_subtask_0_subtask_3\n",
      "Creating task for previous result: task_0_subtask_0_subtask_4\n",
      "Creating task for previous result: task_0_subtask_0_subtask_5\n",
      "Creating task for previous result: task_0_subtask_0_subtask_6\n",
      "Creating task for previous result: task_0_subtask_0_subtask_7\n",
      "Creating task for previous result: task_0_subtask_1_subtask_0\n",
      "Creating task for previous result: task_0_subtask_1_subtask_1\n",
      "Creating task for previous result: task_0_subtask_1_subtask_2\n",
      "Creating task for previous result: task_0_subtask_1_subtask_3\n",
      "Creating task for previous result: task_0_subtask_1_subtask_4\n",
      "Creating task for previous result: task_0_subtask_2_subtask_0\n",
      "Creating task for previous result: task_0_subtask_2_subtask_1\n",
      "Creating task for previous result: task_0_subtask_2_subtask_2\n",
      "Creating task for previous result: task_0_subtask_2_subtask_3\n",
      "Creating task for previous result: task_0_subtask_2_subtask_4\n",
      "Creating task for previous result: task_0_subtask_2_subtask_5\n",
      "Creating task for previous result: task_0_subtask_3_subtask_0\n",
      "Creating task for previous result: task_0_subtask_3_subtask_1\n",
      "Creating task for previous result: task_0_subtask_3_subtask_2\n",
      "Creating task for previous result: task_0_subtask_3_subtask_3\n",
      "Creating task for previous result: task_0_subtask_3_subtask_4\n",
      "Creating task for previous result: task_0_subtask_3_subtask_5\n",
      "Creating task for previous result: task_0_subtask_4_subtask_0\n",
      "Creating task for previous result: task_0_subtask_4_subtask_1\n",
      "Creating task for previous result: task_0_subtask_4_subtask_2\n",
      "Creating task for previous result: task_0_subtask_4_subtask_3\n",
      "Creating task for previous result: task_0_subtask_4_subtask_4\n",
      "Creating task for previous result: task_0_subtask_4_subtask_5\n",
      "Creating task for previous result: task_0_subtask_4_subtask_6\n",
      "Creating task for previous result: task_0_subtask_4_subtask_7\n",
      "Creating task for previous result: task_0_subtask_5_subtask_0\n",
      "Creating task for previous result: task_0_subtask_5_subtask_1\n",
      "Creating task for previous result: task_0_subtask_5_subtask_2\n",
      "Creating task for previous result: task_0_subtask_5_subtask_3\n",
      "Creating task for previous result: task_0_subtask_5_subtask_4\n",
      "Creating task for previous result: task_0_subtask_5_subtask_5\n",
      "Creating task for previous result: task_0_subtask_5_subtask_6\n",
      "Creating task for previous result: task_0_subtask_5_subtask_7\n",
      "Executing agent: test_case_generator_agent\n",
      "Initialized results structure for agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Executing agent: test_case_generator_agent\n",
      "Tracking execution under parent agent: test_case_family_agent\n",
      "Running agent with previous result from: test_case_family_agent\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_0_subtask_6\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_4_subtask_7\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_0_subtask_5\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_5_subtask_4\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_5_subtask_6\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_1_subtask_0\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_1_subtask_2\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_3_subtask_4\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_0_subtask_4\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_0_subtask_0\n",
      "Completed task: task_0_subtask_0_subtask_0\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_4_subtask_3\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_3_subtask_1\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_0_subtask_7\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_5_subtask_3\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_4_subtask_5\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_2_subtask_3\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_3_subtask_0\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_0_subtask_3\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_5_subtask_5\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_2_subtask_4\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_0_subtask_2\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_1_subtask_4\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_2_subtask_5\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_5_subtask_1\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_4_subtask_0\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_0_subtask_1\n",
      "Completed task: task_0_subtask_0_subtask_1\n",
      "Completed task: task_0_subtask_0_subtask_2\n",
      "Completed task: task_0_subtask_0_subtask_3\n",
      "Completed task: task_0_subtask_0_subtask_4\n",
      "Completed task: task_0_subtask_0_subtask_5\n",
      "Completed task: task_0_subtask_0_subtask_6\n",
      "Completed task: task_0_subtask_0_subtask_7\n",
      "Completed task: task_0_subtask_1_subtask_0\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_5_subtask_0\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_2_subtask_1\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_2_subtask_2\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_3_subtask_5\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_4_subtask_6\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_5_subtask_7\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_1_subtask_3\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_5_subtask_2\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_4_subtask_2\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_4_subtask_4\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_1_subtask_1\n",
      "Completed task: task_0_subtask_1_subtask_1\n",
      "Completed task: task_0_subtask_1_subtask_2\n",
      "Completed task: task_0_subtask_1_subtask_3\n",
      "Completed task: task_0_subtask_1_subtask_4\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_2_subtask_0\n",
      "Completed task: task_0_subtask_2_subtask_0\n",
      "Completed task: task_0_subtask_2_subtask_1\n",
      "Completed task: task_0_subtask_2_subtask_2\n",
      "Completed task: task_0_subtask_2_subtask_3\n",
      "Completed task: task_0_subtask_2_subtask_4\n",
      "Completed task: task_0_subtask_2_subtask_5\n",
      "Completed task: task_0_subtask_3_subtask_0\n",
      "Completed task: task_0_subtask_3_subtask_1\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_3_subtask_3\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_3_subtask_2\n",
      "Completed task: task_0_subtask_3_subtask_2\n",
      "Completed task: task_0_subtask_3_subtask_3\n",
      "Completed task: task_0_subtask_3_subtask_4\n",
      "Completed task: task_0_subtask_3_subtask_5\n",
      "Completed task: task_0_subtask_4_subtask_0\n",
      "Stored result for key: test_case_family_agent_level_2_task_0_subtask_4_subtask_1\n",
      "Completed task: task_0_subtask_4_subtask_1\n",
      "Completed task: task_0_subtask_4_subtask_2\n",
      "Completed task: task_0_subtask_4_subtask_3\n",
      "Completed task: task_0_subtask_4_subtask_4\n",
      "Completed task: task_0_subtask_4_subtask_5\n",
      "Completed task: task_0_subtask_4_subtask_6\n",
      "Completed task: task_0_subtask_4_subtask_7\n",
      "Completed task: task_0_subtask_5_subtask_0\n",
      "Completed task: task_0_subtask_5_subtask_1\n",
      "Completed task: task_0_subtask_5_subtask_2\n",
      "Completed task: task_0_subtask_5_subtask_3\n",
      "Completed task: task_0_subtask_5_subtask_4\n",
      "Completed task: task_0_subtask_5_subtask_5\n",
      "Completed task: task_0_subtask_5_subtask_6\n",
      "Completed task: task_0_subtask_5_subtask_7\n",
      "Expanding 4 results from task: task_0_subtask_0_subtask_0\n",
      "Expanding 2 results from task: task_0_subtask_0_subtask_1\n",
      "Expanding 6 results from task: task_0_subtask_0_subtask_2\n",
      "Expanding 2 results from task: task_0_subtask_0_subtask_3\n",
      "Expanding 2 results from task: task_0_subtask_0_subtask_4\n",
      "Expanding 2 results from task: task_0_subtask_0_subtask_5\n",
      "Expanding 2 results from task: task_0_subtask_0_subtask_6\n",
      "Expanding 2 results from task: task_0_subtask_0_subtask_7\n",
      "Expanding 4 results from task: task_0_subtask_1_subtask_0\n",
      "Expanding 4 results from task: task_0_subtask_1_subtask_1\n",
      "Expanding 4 results from task: task_0_subtask_1_subtask_2\n",
      "Expanding 4 results from task: task_0_subtask_1_subtask_3\n",
      "Expanding 4 results from task: task_0_subtask_1_subtask_4\n",
      "Expanding 6 results from task: task_0_subtask_2_subtask_0\n",
      "Expanding 5 results from task: task_0_subtask_2_subtask_1\n",
      "Expanding 6 results from task: task_0_subtask_2_subtask_2\n",
      "Expanding 7 results from task: task_0_subtask_2_subtask_3\n",
      "Expanding 5 results from task: task_0_subtask_2_subtask_4\n",
      "Expanding 5 results from task: task_0_subtask_2_subtask_5\n",
      "Expanding 5 results from task: task_0_subtask_3_subtask_0\n",
      "Expanding 5 results from task: task_0_subtask_3_subtask_1\n",
      "Expanding 5 results from task: task_0_subtask_3_subtask_2\n",
      "Expanding 2 results from task: task_0_subtask_3_subtask_3\n",
      "Expanding 3 results from task: task_0_subtask_3_subtask_4\n",
      "Expanding 6 results from task: task_0_subtask_3_subtask_5\n",
      "Expanding 7 results from task: task_0_subtask_4_subtask_0\n",
      "Expanding 6 results from task: task_0_subtask_4_subtask_1\n",
      "Expanding 6 results from task: task_0_subtask_4_subtask_2\n",
      "Expanding 2 results from task: task_0_subtask_4_subtask_3\n",
      "Expanding 6 results from task: task_0_subtask_4_subtask_4\n",
      "Expanding 3 results from task: task_0_subtask_4_subtask_5\n",
      "Expanding 5 results from task: task_0_subtask_4_subtask_6\n",
      "Expanding 2 results from task: task_0_subtask_4_subtask_7\n",
      "Expanding 3 results from task: task_0_subtask_5_subtask_0\n",
      "Expanding 2 results from task: task_0_subtask_5_subtask_1\n",
      "Expanding 5 results from task: task_0_subtask_5_subtask_2\n",
      "Expanding 2 results from task: task_0_subtask_5_subtask_3\n",
      "Expanding 2 results from task: task_0_subtask_5_subtask_4\n",
      "Expanding 2 results from task: task_0_subtask_5_subtask_5\n",
      "Expanding 2 results from task: task_0_subtask_5_subtask_6\n",
      "Expanding 4 results from task: task_0_subtask_5_subtask_7\n",
      "Level 2 completed with 161 expanded results\n",
      "\n",
      "All levels completed\n"
     ]
    }
   ],
   "source": [
    "orchestrator = AgentOrchestrator(\n",
    "    [\n",
    "        (user_modelling_agent, {\"user_prompt\": \"Generate test cases for API spec: \" + str(api_spec)}),\n",
    "        (test_case_family_agent, {\"user_prompt\": \"Generate the test case families for this user persona: \"}),\n",
    "        (test_case_generator_agent, {\"user_prompt\": \"Expand the test case family of tests: \"}),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run all agents in parallel\n",
    "results = await orchestrator.run_parallel(\n",
    "    # deps=MyDeps(known_users=\"A young man who wants to adopt a pet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentResult(status=<AgentStatus.COMPLETED: 'completed'>,\n",
      "            data=[UserPersona(persona_type='User', persona='Pet Owner', primary_intentions='Retrieve information about pets based on their status (available, adopted, etc.)', secondary_intentions='Understand pet characteristics relevant for adoption.'),\n",
      "                  UserPersona(persona_type='User', persona='API Developer', primary_intentions='Test the API behavior with various status values.', secondary_intentions='Ensure proper error messages are returned for invalid inputs.'),\n",
      "                  UserPersona(persona_type='User', persona='System Tester', primary_intentions='Validate the response structure for GET requests to /pets.', secondary_intentions='Check the performance and response time of the GET operation under load.'),\n",
      "                  UserPersona(persona_type='User', persona='Data Analyst', primary_intentions='Analyze the data retrieved about pets to derive insights.', secondary_intentions='Cross-reference pet data with external datasets for analysis.'),\n",
      "                  UserPersona(persona_type='Service', persona='Logging Service', primary_intentions='Capture API request and response logs for GET and POST requests.', secondary_intentions='Monitor API usage statistics and error rates.'),\n",
      "                  UserPersona(persona_type='Service', persona='Validation Service', primary_intentions='Validate the request body schema for POST requests to /pets.', secondary_intentions='Check for business logic errors in the submitted data.')],\n",
      "            msg=None)\n"
     ]
    }
   ],
   "source": [
    "pprint(results[\"user_modelling_agent\"][\"user_modelling_agent_level_0_task_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentResult(status=<AgentStatus.COMPLETED: 'completed'>,\n",
      "            data=[TestCaseFamily(name='Retrieve Available Pets', description='Test the retrieval of available pets from the database using a standard parameter.', test_case_type='Normal', test_variations=[\"Request with valid status 'available'\", 'Request multiple statuses at once (available, adopted)']),\n",
      "                  TestCaseFamily(name='Retrieve Adopted Pets', description='Test the retrieval of adopted pets by using the status filter.', test_case_type='Normal', test_variations=[\"Request with valid status 'adopted'\", 'Request for specific breed in adopted pets']),\n",
      "                  TestCaseFamily(name='Filter by Pet Characteristics', description='Validate the ability to filter pets by characteristics such as age, size, and type.', test_case_type='Normal', test_variations=[\"Request filtering by age '2 years'\", \"Request filtering by size 'small' or 'large'\"]),\n",
      "                  TestCaseFamily(name='Invalid Status Request', description='Check behavior when an invalid status is requested.', test_case_type='Edge', test_variations=[\"Request with status 'not-a-status'\", 'Request with multiple invalid statuses']),\n",
      "                  TestCaseFamily(name='Empty Response for Status', description='Simulate an empty response when no pets match the requested status.', test_case_type='Edge', test_variations=[\"Request status 'available' when all pets are adopted\", \"Request status 'adopted' when no pets have been adopted\"]),\n",
      "                  TestCaseFamily(name='Missing Parameters', description='Test system behavior when required parameters are missing from the request.', test_case_type='Edge', test_variations=['Request without any parameters', \"Request with only the field 'size'\"]),\n",
      "                  TestCaseFamily(name='Stress Test Multiple Requests', description='Simulate high load by making multiple simultaneous requests for pet information.', test_case_type='Stress', test_variations=['10 concurrent requests for available pets', '100 requests in rapid succession for adopted pets']),\n",
      "                  TestCaseFamily(name='Large Data Set Retrieval', description='Test retrieval of a large dataset of pets to check system performance.', test_case_type='Stress', test_variations=['Request details for 1000 pets at once', 'Request with pagination for 1000 pets'])],\n",
      "            msg=None)\n"
     ]
    }
   ],
   "source": [
    "pprint(results[\"test_case_family_agent\"][\"user_modelling_agent_level_1_task_0_subtask_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_case_family_agent_level_2_task_0_subtask_0_subtask_0': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Retrieve Available Pets - Valid Request', description=\"Test retrieving pets that are currently marked as 'available' in the database.\", input_json={'status': 'available'}, expected_output_prompt='Expect a list of pets that are available with their details such as name, breed, and age.', expected_output_json={'pets': [{'name': 'Buddy', 'breed': 'Labrador', 'age': 3}, {'name': 'Mittens', 'breed': 'Siamese', 'age': 2}]}, preconditions=None), TestCase(name='Retrieve Available Pets - Multiple Statuses', description=\"Test retrieving pets by requesting multiple statuses at once, including 'available' and 'adopted'.\", input_json={'status': ['available', 'adopted']}, expected_output_prompt=\"Expect a list of pets with status 'available' and those that are 'adopted'.\", expected_output_json={'pets': [{'name': 'Buddy', 'breed': 'Labrador', 'age': 3, 'status': 'available'}, {'name': 'Whiskers', 'breed': 'Tabby', 'age': 4, 'status': 'adopted'}]}, preconditions=None), TestCase(name='Retrieve Available Pets - No Pets Available', description='Test the API behavior when requesting pets with a status that has no available pets.', input_json={'status': 'unavailable'}, expected_output_prompt='Expect an empty list of pets and a message indicating no pets are available for the given status.', expected_output_json={'pets': [], 'message': 'No pets available for the requested status.'}, preconditions=None), TestCase(name='Retrieve Available Pets - Invalid Status', description='Test the API behavior when requesting pets with an invalid status.', input_json={'status': 'unknown'}, expected_output_prompt='Expect an error message indicating the status is invalid.', expected_output_json={'error': 'Invalid status provided.'}, preconditions=None)], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_0_subtask_1': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Retrieve Adopted Pets - Valid Status', description=\"Ensure that the API retrieves all pets with the status 'adopted'.\", input_json=None, expected_output_prompt='List of adopted pets should be provided.', expected_output_json=None, preconditions=\"There are pets in the database with the status 'adopted'.\"), TestCase(name='Retrieve Adopted Pets - Filter by Breed', description='Test pet retrieval by filtering for a specific breed among adopted pets.', input_json={'status': 'adopted', 'breed': 'Labrador'}, expected_output_prompt='List of adopted pets of breed Labrador should be returned.', expected_output_json=None, preconditions=\"At least one Labrador exists in the database with status 'adopted'.\")], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_0_subtask_2': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Filter by Pet Characteristics - Age 2 Years', description='Request filtering of pets to retrieve only those that are 2 years old.', input_json=None, expected_output_prompt='A list of pets filtered to show only those that are 2 years old.', expected_output_json=None, preconditions='The database contains at least one pet that is 2 years old. The API endpoint for filtering is properly configured.'), TestCase(name='Filter by Pet Characteristics - Size Small', description='Request filtering of pets to retrieve only those that are categorized as small.', input_json=None, expected_output_prompt='A list of pets filtered to show only those that are small in size.', expected_output_json=None, preconditions='The database contains at least one small pet. The API endpoint for filtering is properly configured.'), TestCase(name='Filter by Pet Characteristics - Size Large', description='Request filtering of pets to retrieve only those that are categorized as large.', input_json=None, expected_output_prompt='A list of pets filtered to show only those that are large in size.', expected_output_json=None, preconditions='The database contains at least one large pet. The API endpoint for filtering is properly configured.'), TestCase(name='Filter by Pet Characteristics - Invalid Age', description='Request filtering of pets with an invalid age input, such as a negative number or non-numeric value.', input_json=None, expected_output_prompt='Error message indicating invalid age format.', expected_output_json=None, preconditions='The API must contain error handling for invalid input formats.'), TestCase(name='Filter by Pet Characteristics - Mixed Characteristics', description=\"Request filtering by combining multiple characteristics such as age '2 years' and size 'small'.\", input_json=None, expected_output_prompt='A list of pets filtered by both characteristics, showing only pets that are 2 years old and small.', expected_output_json=None, preconditions='The database contains at least one pet that is 2 years old and small. The API must support filtering by multiple characteristics.'), TestCase(name='Filter by Pet Characteristics - No Matches Found', description=\"Request filtering of pets by characteristics that do not exist in the database, such as '5 years and medium size'.\", input_json=None, expected_output_prompt='Empty list or a message indicating no pets met the filter criteria.', expected_output_json=None, preconditions='The database does not contain any pets that match the specified characteristics.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_0_subtask_3': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Invalid Status Request - Single Invalid Status', description=\"Check behavior when a single invalid status 'not-a-status' is requested.\", input_json={'status': 'not-a-status'}, expected_output_prompt='Return a 400 Bad Request with an appropriate error message.', expected_output_json={'error': 'Invalid status requested.'}, preconditions='Ensure the API is up and running.'), TestCase(name='Invalid Status Request - Multiple Invalid Statuses', description='Check behavior when multiple invalid statuses are requested.', input_json={'statuses': ['not-a-status1', 'not-a-status2']}, expected_output_prompt='Return a 400 Bad Request with an appropriate error message for each invalid status.', expected_output_json={'errors': ['Invalid status requested: not-a-status1', 'Invalid status requested: not-a-status2']}, preconditions='Ensure the API is up and running.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_0_subtask_4': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Empty Response for Status - Available', description=\"Request pets with the status 'available' when there are no available pets in the inventory, expecting an empty response.\", input_json=None, expected_output_prompt='Expect an empty array as there are no pets available.', expected_output_json=None, preconditions='All pets are adopted.'), TestCase(name='Empty Response for Status - Adopted', description=\"Request pets with the status 'adopted' when there are no adopted pets recorded, expecting an empty response.\", input_json=None, expected_output_prompt='Expect an empty array as there are no adopted pets.', expected_output_json=None, preconditions='No pets have been adopted.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_0_subtask_5': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Missing Parameters - No Parameters', description='Send a request without any parameters and expect an error response indicating missing required fields.', input_json={}, expected_output_prompt='Expected response should indicate that required parameters are missing, with error code 400.', expected_output_json={'error': 'Missing required parameters', 'code': 400}, preconditions='The API endpoint being tested must be operational.'), TestCase(name='Missing Parameters - Only Size Parameter', description=\"Send a request with only the 'size' parameter and expect an error response for the missing required parameters.\", input_json={'size': 10}, expected_output_prompt='Expected response should indicate that other required parameters are missing, with error code 400.', expected_output_json={'error': 'Missing required parameters: name, type', 'code': 400}, preconditions=\"The API endpoint being tested must recognize that 'size' is insufficient without other specified fields.\")], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_0_subtask_6': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Stress Test Multiple Requests - 10 Concurrent', description=\"Simulate high load by making 10 simultaneous requests for available pets to check the system's handling of concurrent connections.\", input_json=None, expected_output_prompt='The system should return status 200 with valid data for all 10 requests or indicate any failures without crashing.', expected_output_json=None, preconditions=None), TestCase(name='Stress Test Multiple Requests - 100 Rapid Succession', description='Make 100 rapid requests in succession for adopted pets to test the API response time and server load management.', input_json=None, expected_output_prompt='The system should maintain response times below 2 seconds for all requests, returning appropriate not found statuses where applicable without latency spikes.', expected_output_json=None, preconditions=None)], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_0_subtask_7': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Large Data Set Retrieval - Bulk Request', description=\"Request details for 1000 pets at once to test the system's ability to handle large datasets in a single request and check for performance issues.\", input_json=None, expected_output_prompt='The system should successfully return a list of 1000 pets without errors, with correct data representation and appropriate response time.', expected_output_json=None, preconditions='The database must contain a minimum of 1000 pet records.'), TestCase(name='Large Data Set Retrieval - Paginated Request', description='Request with pagination for 1000 pets to verify if fetching large datasets through pagination maintains performance and correctness.', input_json=None, expected_output_prompt='The first page should contain up to the specified number of pets (e.g., 100), and subsequent pages should return the correct remaining pets until all are fetched, ensuring proper pagination logic.', expected_output_json=None, preconditions='The database must be populated with at least 1000 pet records.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_1_subtask_0': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Normal API Status Testing - GET Request', description='Verify that the API responds with status code 200 for a valid GET request.', input_json=None, expected_output_prompt='HTTP Status Code: 200, Response Body: Expected Data', expected_output_json=None, preconditions=None), TestCase(name='Normal API Status Testing - POST Request', description='Verify that the API responds with status code 201 for a valid POST request to create a resource.', input_json=None, expected_output_prompt='HTTP Status Code: 201, Response Body: Created Resource ID', expected_output_json=None, preconditions=None), TestCase(name='Normal API Status Testing - PUT Request', description='Verify that the API responds with status code 204 for a valid PUT request to update a resource.', input_json=None, expected_output_prompt='HTTP Status Code: 204, No Content', expected_output_json=None, preconditions=None), TestCase(name='Normal API Status Testing - DELETE Request', description='Verify that the API responds with status code 204 for a valid DELETE request to remove a resource.', input_json=None, expected_output_prompt='HTTP Status Code: 204, No Content', expected_output_json=None, preconditions=None)], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_1_subtask_1': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Invalid Status Code GET Request', description='Send a GET request with an invalid status code (e.g., 999) and check for error handling.', input_json=None, expected_output_prompt='Expect API to respond with a 400 Bad Request error and a message indicating invalid status code.', expected_output_json=None, preconditions='API endpoint must be accessible.'), TestCase(name='Missing Required Fields POST Request', description='Send a POST request that omits required fields and check for appropriate error messages.', input_json={'name': '', 'email': 'test@example.com'}, expected_output_prompt='Expect API to respond with a 400 Bad Request error and a message indicating which required fields are missing.', expected_output_json=None, preconditions=\"API must require 'name' and 'email' fields.\"), TestCase(name='Malformed JSON Payload PUT Request', description='Send a PUT request with a malformed JSON payload to check the error handling.', input_json={'name': 'Test User', 'email': 'test@example.com'}, expected_output_prompt='Expect API to respond with a 400 Bad Request error and a message indicating JSON format issues.', expected_output_json=None, preconditions='API must accept valid JSON and throw an error for malformed JSON.'), TestCase(name='Invalid Resource ID DELETE Request', description='Send a DELETE request with an invalid resource ID to ensure error handling is effective.', input_json=None, expected_output_prompt='Expect API to respond with a 404 Not Found error and a message indicating the resource ID does not exist.', expected_output_json=None, preconditions='API resource ID provided should not exist.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_1_subtask_2': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Boundary Status Value 100', description=\"Testing API's handling of information responses with status code 100.\", input_json=None, expected_output_prompt='Expecting the API to return no content with status 100 and appropriate headers indicating continue.', expected_output_json=None, preconditions='The API should be designed to acknowledge the 100 status code.'), TestCase(name='Boundary Status Value 199', description=\"Testing API's handling of non-standard responses with status code 199.\", input_json=None, expected_output_prompt='Expecting the API to return a client error response, as 199 is not a valid response code.', expected_output_json=None, preconditions='The API should implement handling for invalid status codes.'), TestCase(name='Boundary Status Value 300', description=\"Testing API's handling of multiple choices with status code 300.\", input_json=None, expected_output_prompt='Expecting the API to provide a list of available options as part of the response.', expected_output_json=None, preconditions='The API should correctly handle redirect responses.'), TestCase(name='Boundary Status Value 400', description=\"Testing API's handling of client errors with status code 400.\", input_json=None, expected_output_prompt='Expecting the API to return a clear error message indicating a bad request.', expected_output_json=None, preconditions='The API should correctly recognize 400 as a client error.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_1_subtask_3': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Stress Test - 100 GET Requests in 1 Second', description='Simulate sending 100 GET requests to the API within a 1-second time frame to assess how the system handles a surge of incoming requests.', input_json=None, expected_output_prompt='Expect response times of under 200ms for each GET request and no server errors (5xx errors) should be encountered.', expected_output_json=None, preconditions='The API is operational and capable of handling high traffic.'), TestCase(name='Stress Test - 50 Simultaneous POST Requests', description=\"Test the API's ability to handle 50 POST requests sent simultaneously, assessing how the system processes multiple payloads at once.\", input_json=None, expected_output_prompt='Expect all 50 POST requests to be processed successfully with status 201 created and no server errors (5xx errors).', expected_output_json=None, preconditions='The server is setup to accept simultaneous POST requests.'), TestCase(name='Stress Test - 200 Requests with Varying Status Codes', description='Conduct a test by sending 200 requests where each request is designed to elicit different status codes (200, 400, 404, 500) to evaluate how the system deals with different scenarios.', input_json=None, expected_output_prompt='Expect a distribution of status codes as per the requests sent, with no more than 1% of requests resulting in server errors (5xx errors).', expected_output_json=None, preconditions='The API can respond with a variety of status codes.'), TestCase(name='Stress Test - Concurrent DELETE Requests', description='Execute concurrent DELETE requests for multiple resources, aiming to test the load handling and response management of the API.', input_json=None, expected_output_prompt='Expect all DELETE requests to return status 204 No Content indicating successful deletions, and no server errors encountered.', expected_output_json=None, preconditions='Resources to delete exist and can be accessed.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_1_subtask_4': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='POST request with numeric status instead of string', description='Testing if the API correctly identifies and handles a numeric status instead of an expected string in a POST request.', input_json={'status': 200}, expected_output_prompt='The API should return a validation error indicating the expected data type for the status field.', expected_output_json={'error': \"Invalid data type for field 'status'. Expected string.\"}, preconditions='The API endpoint is set up to receive a POST request for creating a resource.'), TestCase(name='PUT request with boolean value in a text field', description=\"Testing the API's response when a boolean value is sent instead of text in a PUT request.\", input_json={'name': True}, expected_output_prompt='The API should return a validation error indicating the name field must be a string.', expected_output_json={'error': \"Invalid data type for field 'name'. Expected string.\"}, preconditions='The API endpoint is set up to update a resource with a PUT request.'), TestCase(name='DELETE request with a string ID instead of numerical', description='Test how the API handles a DELETE request when a string is sent as an ID rather than a numerical value.', input_json={'id': 'abc123'}, expected_output_prompt='The API should return a validation error stating the ID must be a number.', expected_output_json={'error': 'Invalid ID type. ID must be a number.'}, preconditions='The API endpoint is available for deleting resources via DELETE request.'), TestCase(name='GET request with a status parameter set to null', description='Assess the response of the API when a GET request includes a null value for a required status parameter.', input_json={'status': None}, expected_output_prompt=\"The API should return a validation error indicating that the status parameter can't be null.\", expected_output_json={'error': \"Invalid value for parameter 'status'. It cannot be null.\"}, preconditions='The API endpoint is configured to allow retrieval of resources with status filtering.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_2_subtask_0': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate GET /pets response structure - Normal Flow', description='Verify that the response structure for a successful GET request to /pets contains all expected fields (id, name, type, age).', input_json=None, expected_output_prompt=None, expected_output_json=None, preconditions='User has permission to access pet data.'), TestCase(name='Validate GET /pets response structure - Edge Case Missing Field', description=\"Verify that the response handles the case where one of the expected fields (e.g., 'age') is missing from the response payload.\", input_json=None, expected_output_prompt='Check if the response gracefully handles missing fields and returns a meaningful error message.', expected_output_json=None, preconditions='User makes a GET request to /pets and a response is generated with incomplete data.'), TestCase(name='Validate GET /pets response structure - Edge Case Extra Field', description=\"Check how the system handles a response that includes an unexpected additional field in the response payload (e.g., 'color').\", input_json=None, expected_output_prompt='Verify that the presence of the extra field does not affect the correctness of the response and that core fields are intact.', expected_output_json=None, preconditions='User makes a GET request to /pets and a response is generated with an unexpected field.'), TestCase(name='Validate GET /pets response structure - Edge Case Data Type', description=\"Ensure the system handles the scenario where the response fields are of incorrect data types (e.g., 'age' is a string instead of an integer).\", input_json=None, expected_output_prompt='Check if the API responds with a validation error or if it processes the incorrect data type without an error.', expected_output_json=None, preconditions='User makes a GET request to /pets and a response is generated with incorrect data types for response fields.'), TestCase(name='Validate GET /pets response structure - Stress Case High Load', description='Simulate a high number of concurrent GET requests to /pets to assess performance under load.', input_json=None, expected_output_prompt='Monitor response time and resource usage to ensure the service can handle the load without errors.', expected_output_json=None, preconditions='User initiates 100 concurrent GET requests to /pets.'), TestCase(name='Validate GET /pets response structure - Stress Case Massive Dataset', description='Test response performance and structure when the /pets endpoint returns a very large dataset.', input_json=None, expected_output_prompt='Ensure the response is correctly formatted and all expected fields are present even with a large dataset.', expected_output_json=None, preconditions='User makes a GET request to /pets that returns thousands of pet records.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_2_subtask_1': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate GET /pets response structure - Missing Name Field', description=\"Send a GET request to /pets where the response intentionally does not include the 'name' field. Ensure a proper error response is returned indicating that the 'name' field is required.\", input_json=None, expected_output_prompt=\"Expect error indicating missing 'name' field or 400 Bad Request\", expected_output_json=None, preconditions=None), TestCase(name='Validate GET /pets response structure - Missing Age Field', description=\"Send a GET request to /pets where the response intentionally does not include the 'age' field. Ensure a proper error response is returned indicating that the 'age' field is required.\", input_json=None, expected_output_prompt=\"Expect error indicating missing 'age' field or 400 Bad Request\", expected_output_json=None, preconditions=None), TestCase(name='Validate GET /pets response structure - Missing Type Field', description=\"Send a GET request to /pets where the response intentionally does not include the 'type' field. Ensure a proper error response is returned indicating that the 'type' field is required.\", input_json=None, expected_output_prompt=\"Expect error indicating missing 'type' field or 400 Bad Request\", expected_output_json=None, preconditions=None), TestCase(name='Validate GET /pets response structure - Multiple Missing Fields', description=\"Send a GET request to /pets where the response intentionally misses multiple fields (e.g., 'name' and 'age'). Ensure the error response indicates all missing fields.\", input_json=None, expected_output_prompt='Expect error indicating multiple missing fields or 400 Bad Request', expected_output_json=None, preconditions=None), TestCase(name='Validate GET /pets response structure - Missing Owner Field', description=\"Send a GET request to /pets where the response intentionally does not include the 'owner' field. Ensure a proper error response is returned indicating that the 'owner' field is required.\", input_json=None, expected_output_prompt=\"Expect error indicating missing 'owner' field or 400 Bad Request\", expected_output_json=None, preconditions=None)], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_2_subtask_2': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate GET /pets with Invalid Query Parameter - Unrecognized Parameter', description='Test the response structure when an unrecognized query parameter is supplied in the GET request.', input_json=None, expected_output_prompt='Expect 400 Bad Request response', expected_output_json=None, preconditions='The API is running and reachable.'), TestCase(name='Validate GET /pets with Invalid Query Parameter - Missing Value', description='Test the response structure when an invalid query parameter is provided without a value in the GET request.', input_json=None, expected_output_prompt='Expect 400 Bad Request response', expected_output_json=None, preconditions='The API is running and reachable.'), TestCase(name='Validate GET /pets with Invalid Query Parameter - SQL Injection Attempt', description='Test the response structure when potential SQL injection is detected in the query parameter of the GET request.', input_json=None, expected_output_prompt='Expect 400 Bad Request response', expected_output_json=None, preconditions='The API is running and reachable.'), TestCase(name='Validate GET /pets with Invalid Query Parameter - Special Characters', description='Test the response structure when special characters are included in the query parameter of the GET request.', input_json=None, expected_output_prompt='Expect 400 Bad Request response', expected_output_json=None, preconditions='The API is running and reachable.'), TestCase(name='Validate GET /pets with Invalid Query Parameter - Very Long Parameter Name', description='Test the response structure when an excessively long query parameter name is provided in the GET request.', input_json=None, expected_output_prompt='Expect 400 Bad Request response', expected_output_json=None, preconditions='The API is running and reachable.'), TestCase(name='Validate GET /pets with Invalid Query Parameter - Multiple Invalid Parameters', description='Test the response structure when multiple invalid query parameters are included in the GET request.', input_json=None, expected_output_prompt='Expect 400 Bad Request response', expected_output_json=None, preconditions='The API is running and reachable.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_2_subtask_3': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Test GET /pets under normal load', description='Measure response time for GET /pets when sent 100 concurrent requests to check performance under standard load.', input_json=None, expected_output_prompt=None, expected_output_json=None, preconditions=None), TestCase(name='Test GET /pets under increased load', description='Measure response time for GET /pets when sent 500 concurrent requests to test system performance under an increased load.', input_json=None, expected_output_prompt=None, expected_output_json=None, preconditions='Ensure the server can handle concurrent requests.'), TestCase(name='Test GET /pets with varying response times', description='Simulate different backend response times (0.5s, 1s, 1.5s, 2s) while sending 100 concurrent requests to /pets.', input_json=None, expected_output_prompt=None, expected_output_json=None, preconditions='Implement response time variations on the backend.'), TestCase(name='Test GET /pets with network latency', description='Send 100 concurrent GET requests to /pets while introducing 200ms network latency to observe system performance under degraded network conditions.', input_json=None, expected_output_prompt=None, expected_output_json=None, preconditions='Simulate network latency using tools.'), TestCase(name='Test GET /pets with large payload', description='Send 100 concurrent GET requests to /pets and ensure that the server can handle responses with large payloads (up to 1MB).', input_json=None, expected_output_prompt=None, expected_output_json=None, preconditions='Ensure endpoint can handle large payloads.'), TestCase(name='Test GET /pets with invalid query parameters', description='Send 100 concurrent GET requests to /pets with various invalid query parameters and check system robustness.', input_json=None, expected_output_prompt=None, expected_output_json=None, preconditions=None), TestCase(name='Test GET /pets with session expiration', description='Send 100 concurrent GET requests to /pets, while simulating session expiration partway through the requests, to check how the server handles expired sessions.', input_json=None, expected_output_prompt=None, expected_output_json=None, preconditions='Set up session management to simulate expiration.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_2_subtask_4': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Test GET /pets under extreme load - Concurrent Request Variation', description='Simulate extreme loads by sending 1000 concurrent requests to /pets and verify if the system holds up without crashing. Monitor response times for each request, ensuring that none exceed 5 seconds, and that no errors occur during the test.', input_json=None, expected_output_prompt='Expect no errors and response times under 5 seconds for all requests.', expected_output_json=None, preconditions='System must be deployed and accessible. Able to handle concurrent requests.'), TestCase(name='Test GET /pets under extreme load - Increased Pet Count', description='Test the system with 1000 concurrent requests to /pets; however, first, populate the database with an increased count of 10,000 pets. Verify system performance and response times.', input_json=None, expected_output_prompt='Expect no errors and response times under 5 seconds for all requests, despite increased database size.', expected_output_json=None, preconditions='Database must contain 10,000 pets before the test.'), TestCase(name='Test GET /pets under extreme load - Network Latency Simulation', description='Introduce artificial network latency while sending 1000 concurrent GET requests to /pets. Measure how latency affects response times and error rates.', input_json=None, expected_output_prompt='Expect no errors but monitor response times. Assess performance degradation due to latency.', expected_output_json=None, preconditions='Network latency simulation tool must be active.'), TestCase(name='Test GET /pets under extreme load - Varying Pet Attributes', description='Send 1000 concurrent requests to /pets, where each request filters on different pet attributes (like species, age, and location) to analyze if filtering impacts the system under load.', input_json=None, expected_output_prompt='Expect consistent response times under 5 seconds and no errors, regardless of attribute filtering.', expected_output_json=None, preconditions='Different attributes must be present in the database for filtering.'), TestCase(name='Test GET /pets under extreme load - Concurrent User Simulation', description='Simulate 1000 users concurrently accessing /pets via different clients. Assess how the system performs under user load and request variety.', input_json=None, expected_output_prompt='Ensure no errors occur and response times remain under 5 seconds across all user requests.', expected_output_json=None, preconditions='User interface must route requests to /pets properly.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_2_subtask_5': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate GET /pets with massive dataset - Variation 1', description='Request /pets endpoint with a database containing 10,000 pet entries to evaluate system performance and resource utilization under load.', input_json=None, expected_output_prompt='HTTP status 200 OK, response time under 2 seconds, returns 10,000 pet entries.', expected_output_json=None, preconditions='Database is pre-loaded with 10,000 pet records.'), TestCase(name='Validate GET /pets with massive dataset - Variation 2', description='Request /pets endpoint with a database containing 100,000 pet entries to evaluate system performance with a significantly larger dataset.', input_json=None, expected_output_prompt='HTTP status 200 OK, response time under 5 seconds, returns 100,000 pet entries.', expected_output_json=None, preconditions='Database is pre-loaded with 100,000 pet records.'), TestCase(name='Validate GET /pets with massive dataset - Variation 3', description='Request /pets endpoint with a database containing 1,000,000 pet entries to test system scalability and response time under extreme load.', input_json=None, expected_output_prompt='HTTP status 200 OK, response time under 10 seconds, returns 1,000,000 pet entries.', expected_output_json=None, preconditions='Database is pre-loaded with 1,000,000 pet records.'), TestCase(name='Validate GET /pets with massive dataset - Variation 4', description='Request /pets endpoint and implement a random delay in retrieving data to test how the system handles delayed responses when dealing with large datasets.', input_json=None, expected_output_prompt='HTTP status 200 OK, response may vary but should maintain acceptable limits of under 15 seconds, returns all pet entries available.', expected_output_json=None, preconditions='Database is pre-loaded with 500,000 pet records and random retrieval delay is implemented.'), TestCase(name='Validate GET /pets with massive dataset - Variation 5', description=\"Execute concurrent requests to /pets endpoint while the database holds a massive amount of data to assess the system's ability to handle multiple simultaneous connections without degrading performance.\", input_json=None, expected_output_prompt='HTTP status 200 OK for each request, response time varies but should not exceed 5 seconds for individual requests, maintaining data integrity across all responses.', expected_output_json=None, preconditions='Database is pre-loaded with 100,000 pet records and initialized to handle concurrent requests.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_3_subtask_0': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Analyze Pet Data - Basic Retrieval - Fetch 10 Pets', description='User fetches data for 10 different pets from the database to analyze common attributes.', input_json=None, expected_output_prompt='A JSON array containing basic pet data for 10 pets including species, breed, age, and any other common attributes.', expected_output_json=None, preconditions='User is authenticated and has permission to access pet data.'), TestCase(name='Analyze Pet Data - Basic Retrieval - Filter by Age', description='User fetches pet data filtering results for age, e.g., pets aged 1-5 years.', input_json={'filters': {'age': {'min': 1, 'max': 5}}}, expected_output_prompt='A JSON array containing basic pet data for pets aged between 1 to 5 years, including species, breed, age, and common attributes.', expected_output_json=None, preconditions='User is authenticated and filters follow the specified format.'), TestCase(name='Analyze Pet Data - Basic Retrieval - Filter by Species', description='User fetches pet data filtering results for a specific species, e.g., dogs or cats.', input_json={'filters': {'species': 'Dog'}}, expected_output_prompt='A JSON array containing basic pet data for dogs, including breed, age, and common attributes.', expected_output_json=None, preconditions='User is authenticated and filters follow the specified format.'), TestCase(name='Analyze Pet Data - Basic Retrieval - Filter by Species and Age', description='User fetches pet data filtering results for a specific species and age range.', input_json={'filters': {'species': 'Cat', 'age': {'min': 3, 'max': 7}}}, expected_output_prompt='A JSON array containing basic pet data for cats aged between 3 to 7 years, including breed, age, and common attributes.', expected_output_json=None, preconditions='User is authenticated and filters follow the specified format.'), TestCase(name='Analyze Pet Data - Basic Retrieval - Retrieve in Standard JSON Format', description='User fetches pet data ensuring the response is in standard JSON format.', input_json=None, expected_output_prompt='A valid JSON response containing basic pet data in the standard format with all required fields.', expected_output_json=None, preconditions='User is authenticated and the API supports standard JSON responses.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_3_subtask_1': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Cross-reference Pet Data with Vaccinations Dataset', description='User fetches pet data and matches it with an external vaccinations dataset to find pets in need of vaccinations.', input_json=None, expected_output_prompt='List of pets that require vaccination along with their vaccination history.', expected_output_json=None, preconditions='User has access to the vaccination dataset.'), TestCase(name='Cross-reference Pet Data with Pet Ownership Data', description='User retrieves pet ownership records and correlates them with pet data to ensure accuracy of ownership information.', input_json=None, expected_output_prompt='List of pets associated with their respective owners and discrepancies if any.', expected_output_json=None, preconditions='User has access to the pet ownership dataset.'), TestCase(name='Check for Missing Records When Joining Datasets', description='User checks for missing records in pet data against the external datasets to ensure completeness of information.', input_json=None, expected_output_prompt='List of pets with missing records after joining datasets.', expected_output_json=None, preconditions='Both datasets are available for cross-referencing.'), TestCase(name='Cross-reference Pet Data with Veterinary Visit Records', description='User matches pet data against veterinary visit records to identify pets with overdue check-ups.', input_json=None, expected_output_prompt='List of pets that have not had a veterinary visit in the last year.', expected_output_json=None, preconditions='User has access to the veterinary visit dataset.'), TestCase(name='Cross-reference Pet Data with Microchip Registration Data', description='User compares pet data with microchip registration records to verify registered pets.', input_json=None, expected_output_prompt='List of pets that are not registered in the microchip database.', expected_output_json=None, preconditions='User has access to the microchip registration dataset.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_3_subtask_2': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Analyze with Maximum Payload - Simple Fetch', description='User retrieves the maximum allowable number of pet records (10000) without any filters to ensure system performance can handle basic load.', input_json=None, expected_output_prompt='Return a list of 10000 pet records.', expected_output_json=None, preconditions='User has valid access credentials and the system allows fetching 10000 records.'), TestCase(name='Analyze with Maximum Payload - Basic Filter', description=\"User retrieves the maximum allowable number of pet records (10000) with a basic filter (e.g., species = 'dog'). Monitor response time and system resource utilization.\", input_json=[{'filter': 'species', 'value': 'dog'}], expected_output_prompt='Return a filtered list of 10000 dog pet records.', expected_output_json=None, preconditions='User has valid access credentials and the system allows fetching 10000 records.'), TestCase(name='Analyze with Maximum Payload - Complex Filter', description=\"User retrieves 10000 pet records using complex filter criteria (e.g., species = 'cat' AND age > 5 AND city = 'New York') to test system performance under intricate conditions.\", input_json=[{'filter': 'species', 'value': 'cat'}, {'filter': 'age', 'condition': '>', 'value': 5}, {'filter': 'city', 'value': 'New York'}], expected_output_prompt='Return a filtered list of 10000 cat pet records meeting specified criteria.', expected_output_json=None, preconditions='User has valid access credentials and the system allows fetching 10000 records.'), TestCase(name='Analyze with Maximum Payload - Real-time Analysis', description='User retrieves 10000 pet records and utilizes a real-time analytics feature to perform calculations (e.g., average age, total number of breeds) on the dataset as it is streamed.', input_json=[{'operation': 'streaming_analysis'}, {'fetch_size': 10000}], expected_output_prompt='Return real-time analysis results of the 10000 pet records, including calculated statistics.', expected_output_json=None, preconditions='User has valid access credentials and is using a system capable of real-time analytics.'), TestCase(name='Analyze with Maximum Payload - Concurrent Requests', description='Simulate multiple users retrieving 10000 pet records simultaneously to evaluate system performance and response time under concurrent load.', input_json=[{'request_count': 50}, {'fetch_size': 10000}], expected_output_prompt='Return response times and success statuses for 50 concurrent requests of 10000 pet records.', expected_output_json=None, preconditions='User has valid access credentials and system supports concurrent requests.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_3_subtask_3': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Invalid Content Type', description='User sends a request with an incorrect content type (e.g., text/plain) instead of application/json to see how the API responds.', input_json=None, expected_output_prompt='HTTP 415 Unsupported Media Type', expected_output_json=None, preconditions='User is authenticated and has access to fetch pet data.'), TestCase(name='Malformed JSON Structure', description='User sends a request with a malformed JSON structure (e.g., missing brackets or key-value pairs) to verify API handling of JSON parsing errors.', input_json={'petId': 123, 'petName': 'Fluffy', 'malformed_structure},': None, 'expected_output_prompt': 'HTTP 400 Bad Request - Invalid JSON format', 'expected_output_json': None, 'preconditions': 'User must use a valid access token.'}, expected_output_prompt=None, expected_output_json=None, preconditions='User needs proper authorization.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_3_subtask_4': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Cross-reference with Null Pet IDs', description='User attempts to join datasets where the Pet ID is null, checking if the system handles missing critical identifiers gracefully.', input_json=None, expected_output_prompt='The system should return an appropriate error message indicating the missing Pet ID or handle the situation without crashing.', expected_output_json=None, preconditions='Datasets being joined must contain a Pet ID field.'), TestCase(name='Cross-reference with Incomplete External Datasets', description='User attempts to cross-reference data with external datasets that miss key fields, testing how the system responds to missing information.', input_json=None, expected_output_prompt=\"The system should indicate fields that are missing in the external datasets and not complete the cross-reference operation. It's critical to document which fields are lacking.\", expected_output_json=None, preconditions='External datasets must be structured and contain fields to be cross-referenced with.'), TestCase(name='Filter by Non-existent Fields', description='User tries to filter results based on fields that do not exist in the dataset, examining system handling of invalid filter requests.', input_json=None, expected_output_prompt='The system should return an informative error or warning that states the selected filter fields do not exist in the dataset.', expected_output_json=None, preconditions='A filter condition must be set with fields that are not present in the main dataset.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_3_subtask_5': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='High Concurrent Requests', description='Send 100 requests concurrently to fetch user data and assess response impact.', input_json=None, expected_output_prompt='System should respond within acceptable limits with minimal error rates.', expected_output_json=None, preconditions='User authentication must be valid.'), TestCase(name='Random Request Variations', description='Send 100 concurrent requests with varying types (GET, POST, PUT) to evaluate system adaptability.', input_json=None, expected_output_prompt='System should handle a mix of request types without significant delay.', expected_output_json=None, preconditions='All user permissions must be valid.'), TestCase(name='Response Time Measurement', description='Measure response times for each of the 100 requests and log the statistics for analysis.', input_json=None, expected_output_prompt='Average response time must be below specified thresholds and error rates minimal.', expected_output_json=None, preconditions='System must be under normal operating parameters before the test.'), TestCase(name='Error Rate Monitoring', description='Count the number of successful vs failed requests during the load test to analyze error rates.', input_json=None, expected_output_prompt='Error rates should not exceed predefined thresholds during the test.', expected_output_json=None, preconditions='All service endpoints should be functional.'), TestCase(name='System Resource Utilization Check', description='Monitor CPU and memory usage while processing 100 concurrent requests to identify potential bottlenecks.', input_json=None, expected_output_prompt='Resource usage must remain within expected limits without system slowdowns.', expected_output_json=None, preconditions='System must be in idle state prior to load testing.'), TestCase(name='Latency Analysis', description='Analyze the network latency during the execution of 100 concurrent requests to evaluate external factors.', input_json=None, expected_output_prompt='Latency should remain consistent and within acceptable limits throughout the load test.', expected_output_json=None, preconditions='Network conditions must be stable.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_4_subtask_0': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Capture GET Request Logs - Standard Valid Endpoint', description='Logs the details of a standard GET request to a valid endpoint including URL, headers, and response status.', input_json=None, expected_output_prompt='Log includes timestamp, URL requested, headers received, and HTTP status code 200.', expected_output_json=None, preconditions=\"The service is up and running, and the endpoint '/api/data' exists.\"), TestCase(name='Capture GET Request Logs - Valid Query Parameters', description='Logs the details of a GET request that includes valid query parameters to a valid endpoint.', input_json=None, expected_output_prompt='Log confirms inclusion of query parameters and a response status code 200.', expected_output_json=None, preconditions=\"The service is running, and the endpoint '/api/data' accepts query parameters.\"), TestCase(name='Capture GET Request Logs - No Query Parameters', description='Logs the details of a GET request to a valid endpoint with no query parameters.', input_json=None, expected_output_prompt='Log includes URL without query parameters and HTTP status code 200.', expected_output_json=None, preconditions=\"The service is running and the endpoint '/api/data' exists.\"), TestCase(name='Capture GET Request Logs - Endpoint Not Found', description='Logs the details of a GET request to an invalid endpoint, checking for correct error logging.', input_json=None, expected_output_prompt='Log captures 404 status code and includes the valid URL attempted.', expected_output_json=None, preconditions=\"Service is running but the endpoint '/api/invalid' does not exist.\"), TestCase(name='Capture GET Request Logs - Invalid Headers', description='Logs the result of a GET request with invalid headers, checking for appropriate error logging.', input_json=None, expected_output_prompt='Log includes the invalid header details and an error status code (e.g., 400).', expected_output_json=None, preconditions=\"Service is running and the endpoint '/api/data' exists.\"), TestCase(name='Capture GET Request Logs - High Load GET Requests', description='Simulate high load by sending multiple GET requests to the same endpoint.', input_json=None, expected_output_prompt='Log should capture details for multiple requests and should not exceed rate limits.', expected_output_json=None, preconditions='The service is running and able to handle load testing.'), TestCase(name='Capture GET Request Logs - Rate Limit Exceeded', description='Logs the outcome when too many GET requests are sent in a short period, testing rate limiting.', input_json=None, expected_output_prompt='Log indicates rate limit status and any request rejections (e.g., status code 429).', expected_output_json=None, preconditions='The service employs rate limiting.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_4_subtask_1': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Capture POST Request Logs - Standard JSON', description='Logs details of a standard POST request with a valid JSON payload.', input_json={'url': 'https://api.example.com/data', 'headers': {'Content-Type': 'application/json'}, 'payload': {'name': 'John', 'age': 30}}, expected_output_prompt='Response status should be 200. Log should include URL, headers, payload, and status.', expected_output_json={'status': 200, 'message': 'Request logged successfully'}, preconditions='Ensure the API endpoint is active and accessible.'), TestCase(name='Capture POST Request Logs - Form Data', description='Logs details of a POST request with valid form data.', input_json={'url': 'https://api.example.com/data', 'headers': {'Content-Type': 'application/x-www-form-urlencoded'}, 'payload': 'name=John&age=30'}, expected_output_prompt='Response status should be 200. Log should include URL, headers, payload, and status.', expected_output_json={'status': 200, 'message': 'Request logged successfully'}, preconditions='Ensure the API endpoint is active and accessible.'), TestCase(name='Capture POST Request Logs - Missing Payload', description='Logs details when a POST request is sent without a payload.', input_json={'url': 'https://api.example.com/data', 'headers': {'Content-Type': 'application/json'}, 'payload': None}, expected_output_prompt='Response status should be 400. Log should include URL, headers, and error message.', expected_output_json={'status': 400, 'error': 'Payload cannot be null'}, preconditions='Ensure the API endpoint is active and accessible.'), TestCase(name='Capture POST Request Logs - Invalid Payload Format', description='Logs details of a POST request with an invalid JSON payload format.', input_json={'url': 'https://api.example.com/data', 'headers': {'Content-Type': 'application/json'}, 'payload': '{not: json}'}, expected_output_prompt='Response status should be 400. Log should include URL, headers, and error message.', expected_output_json={'status': 400, 'error': 'Invalid JSON format'}, preconditions='Ensure the API endpoint is active and accessible.'), TestCase(name='Capture POST Request Logs - Unauthorized Access', description='Logs details of a POST request that is unauthorized.', input_json={'url': 'https://api.example.com/data', 'headers': {'Authorization': 'Bearer invalid_token'}, 'payload': {'name': 'John'}}, expected_output_prompt='Response status should be 401. Log should include URL, headers, and error message.', expected_output_json={'status': 401, 'error': 'Unauthorized'}, preconditions='Ensure the API endpoint is active and accessible.'), TestCase(name='Capture POST Request Logs - Large Payload', description='Logs details of a POST request with a very large JSON payload.', input_json={'url': 'https://api.example.com/data', 'headers': {'Content-Type': 'application/json'}, 'payload': {'data': '<1MB of data>'}}, expected_output_prompt='Response status should be 200. Log should include URL, headers, payload size, and status.', expected_output_json={'status': 200, 'message': 'Request logged successfully'}, preconditions='Ensure the API endpoint can handle large payloads.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_4_subtask_2': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='GET Request Leading to 404 Error', description='Simulates a GET request to a non-existent endpoint to trigger a 404 error. This case will help ensure that the error logging mechanism captures the error status correctly and associates it with the correct endpoint.', input_json=None, expected_output_prompt='API should log a 404 error for nonexistent endpoint and return a JSON object indicating the error with a message.', expected_output_json=None, preconditions='The API endpoint does not exist.'), TestCase(name='POST Request Leading to 500 Error', description=\"Simulates a POST request with valid data that results in a server-side error (500). This should test the system's ability to log critical server errors appropriately.\", input_json=None, expected_output_prompt='API should log a 500 error and return a JSON object indicating internal server error with a message.', expected_output_json=None, preconditions='Server-side processing fails, resulting in a 500 error.'), TestCase(name='GET Request with Rate Limiting 429 Error', description='Simulates multiple rapid GET requests to trigger rate-limiting behavior, resulting in a 429 error. This scenario tests if error tracking captures the 429 status appropriately.', input_json=None, expected_output_prompt='API should log a 429 error for too many requests.', expected_output_json=None, preconditions='Client exceeds allowed rate limit.'), TestCase(name='GET Request with Invalid Authentication 401 Error', description='Simulates a GET request sent with invalid authentication credentials that results in a 401 Unauthorized error. This tests if unauthorized attempts are logged effectively.', input_json=None, expected_output_prompt='API should log a 401 error indicating unauthorized access.', expected_output_json=None, preconditions='Request is sent with incorrect or expired authentication token.'), TestCase(name='POST Request with Invalid Data Structure 400 Error', description=\"Simulates a POST request with a malformed payload resulting in a 400 Bad Request error. This validates the logging system's response to client-side error inputs.\", input_json=None, expected_output_prompt='API should log a 400 error indicating bad request with details about the error.', expected_output_json=None, preconditions='Request payload is incorrectly structured.'), TestCase(name='GET Request to an Overloaded Server 503 Error', description='Simulates a GET request when the server is overloaded and unable to handle requests, resulting in a 503 Service Unavailable error. This tests the tracking of server availability issues.', input_json=None, expected_output_prompt='API should log a 503 error indicating service is unavailable due to overload.', expected_output_json=None, preconditions='Server is under heavy load or undergoing maintenance.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_4_subtask_3': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='GET request missing required query parameter', description=\"Simulate a GET request where a required query parameter (e.g., 'userId') is not provided in the URL.\", input_json=None, expected_output_prompt='Response should indicate a missing parameter error with a specific error code or message.', expected_output_json=None, preconditions='The logging system is active and configured to capture errors.'), TestCase(name='POST request with missing payload fields', description=\"Simulate a POST request where one or more required fields (e.g., 'username', 'password') in the payload are absent.\", input_json=None, expected_output_prompt='Response should indicate a missing payload field error with a specific error code or message.', expected_output_json=None, preconditions='The logging system must be set up to capture errors and log details of the request.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_4_subtask_4': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Capture Logs for Invalid GET Request URL', description='Simulates a GET request with an invalid URL format (e.g. missing protocol or improperly encoded characters).', input_json=None, expected_output_prompt='The system should log the invalid request details, including a specific error message indicating the malformed URL.', expected_output_json=None, preconditions='The logging mechanism is enabled.'), TestCase(name='Capture Logs for Missing GET Request Parameters', description='Simulates a GET request missing critical query parameters that the API expects.', input_json=None, expected_output_prompt='The system should log the request details, mentioning the missing parameters in the error message.', expected_output_json=None, preconditions='The logging mechanism is enabled.'), TestCase(name='Capture Logs for Invalid POST Request JSON Format', description='Simulates a POST request with an improperly formatted JSON body (e.g. missing curly braces or incorrect data types).', input_json=None, expected_output_prompt='The system should log the invalid request details, including a specific error message indicating the issue with JSON format.', expected_output_json=None, preconditions='The logging mechanism is enabled.'), TestCase(name='Capture Logs for Restricted POST Request Headers', description=\"Simulates a POST request that contains headers not adhering to the API's policies (e.g. unsupported Content-Type).\", input_json=None, expected_output_prompt='The system should log the request details and provide an error message regarding unsupported headers.', expected_output_json=None, preconditions='The logging mechanism is enabled.'), TestCase(name='Capture Logs for POST Request with Excessive Payload Size', description='Simulates a POST request where the payload exceeds the maximum allowed size, triggering a log entry.', input_json=None, expected_output_prompt='The system should log the request details with an error message about exceeding payload size limit.', expected_output_json=None, preconditions='The logging mechanism is enabled.'), TestCase(name='Capture Logs for Unsupported HTTP Method', description='Simulates a request using an unsupported HTTP method (e.g. PATCH instead of POST/GET).', input_json=None, expected_output_prompt='The system should log the unsupported method request details with an appropriate error message.', expected_output_json=None, preconditions='The logging mechanism is enabled.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_4_subtask_5': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Stress Test for High Load Logging - Simultaneous GET Requests', description='Simulates 100 simultaneous GET requests to log entries, assessing how the logging mechanism performs under high read loads.', input_json=None, expected_output_prompt='All GET requests should be logged successfully with appropriate response times recorded.', expected_output_json=None, preconditions='Logging service must be running and accessible.'), TestCase(name='Stress Test for High Load Logging - Simultaneous POST Requests with Large Payloads', description=\"Sends 100 simultaneous POST requests with large payloads to test the logging system's ability to handle massive data input efficiently.\", input_json=None, expected_output_prompt='All POST requests should be logged, and the system should not crash or lose any logging information. Response times should remain within acceptable limits.', expected_output_json=None, preconditions='Logging service must be capable of processing large payloads and function under high load.'), TestCase(name='Stress Test for High Load Logging - Mixed GET and POST Requests', description=\"Issues 300 mixed GET and POST requests in a single second to evaluate the logging system's performance under extreme conditions and request management.\", input_json=None, expected_output_prompt='All requests should be correctly logged with appropriate indicators of success or failure. System resource usage and response times should be monitored.', expected_output_json=None, preconditions='The logging service must be resilient enough to manage a mix of request types and maintain integrity in logging.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_4_subtask_6': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Weekly Summary of API Calls', description='Fetch the total number of API calls made during the past week, categorized by endpoint.', input_json=None, expected_output_prompt='A summary report showing the total count of API calls per endpoint for the past week, including timestamps of peak usage.', expected_output_json=None, preconditions='User is authenticated and has permission to access usage statistics.'), TestCase(name='Monthly Analysis of Error Rates by Endpoint', description='Retrieve the error rates of API calls for each endpoint over the past month, focusing on 4xx and 5xx status codes.', input_json=None, expected_output_prompt='A detailed report showing the percentage of errors by endpoint, along with the total number of successful and failed requests.', expected_output_json=None, preconditions='User is authenticated and has permission to access error statistics.'), TestCase(name='Historical API Usage Trends', description='Gather usage statistics over a longer period (e.g., past three months) to identify trends and patterns in API usage.', input_json=None, expected_output_prompt='Trends report displaying weekly or monthly averages of API calls, with visual representations (charts/graphs) as applicable.', expected_output_json=None, preconditions='User is authenticated and has permission to access historical usage statistics.'), TestCase(name='Real-Time API Usage Display', description='Show the current number of API calls in real-time and display current errors as they occur.', input_json=None, expected_output_prompt='Real-time dashboard of current API calls and current error counts, updated every minute.', expected_output_json=None, preconditions='User is authenticated and has permission to view real-time stats.'), TestCase(name='Cumulative API Usage Analysis', description='Calculate total API calls made since inception, providing insights into the growth and usage patterns over time.', input_json=None, expected_output_prompt='Total cumulative count of API calls with a breakdown by month and type of usage.', expected_output_json=None, preconditions='User is authenticated and has permission to access cumulative usage statistics.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_4_subtask_7': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Logging GET Request without Authentication', description='Ensure the system logs an attempt to access the GET endpoint without credentials. The request should be made without any Authorization header.', input_json=None, expected_output_prompt='Expected: HTTP 401 Unauthorized response with log entry indicating access attempt without credentials.', expected_output_json=None, preconditions='API is running and accessible.'), TestCase(name='Logging POST Request with Invalid Authentication Token', description='Ensure the system logs an attempt to access the POST endpoint with an invalid token provided in the Authorization header. The token format should be incorrect or expired.', input_json={'Authorization': 'Bearer invalid_token'}, expected_output_prompt='Expected: HTTP 401 Unauthorized response with log entry indicating access attempt with invalid token.', expected_output_json=None, preconditions='API is running and accessible.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_5_subtask_0': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate Schema - Normal Case - Valid JSON Structure', description='Validate a correctly structured POST request body against the expected schema for /pets endpoint with valid JSON formatting.', input_json={'name': 'Fluffy', 'type': 'Cat', 'age': 3, 'owner': 'John Doe'}, expected_output_prompt='Schema validation successful. Request body matches expected structure.', expected_output_json={'status': 'success', 'message': 'Schema validated successfully.'}, preconditions='The API is online, and the /pets endpoint is available.'), TestCase(name='Validate Schema - Normal Case - Correct Data Types', description='Validate a POST request body against the expected schema for /pets endpoint with correct data types for each field.', input_json={'name': 'Buddy', 'type': 'Dog', 'age': 5, 'owner': 12345}, expected_output_prompt='Schema validation failed. Data types do not match expected schema.', expected_output_json={'status': 'error', 'message': \"Invalid data type for 'owner' field. Expected string.\"}, preconditions='The API is online, and the /pets endpoint is available.'), TestCase(name='Validate Schema - Normal Case - Mandatory Fields Included', description='Validate a POST request body against the expected schema for /pets endpoint, ensuring all mandatory fields are included.', input_json={'type': 'Cat', 'age': 2}, expected_output_prompt='Schema validation failed. Mandatory fields are missing.', expected_output_json={'status': 'error', 'message': \"Missing mandatory field 'name'.\"}, preconditions='The API is online, and the /pets endpoint is available.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_5_subtask_1': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate Schema - Edge Case - Minimum Required Fields', description='Validate a POST request with only the minimum required fields filled out to ensure it meets the schema.', input_json={'field1': 'value1', 'field2': ''}, expected_output_prompt='Schema validation should succeed with the minimum required fields and no errors.', expected_output_json={'status': 'success', 'message': 'Schema validated successfully.'}, preconditions='The API is up and running, and all required fields are known.'), TestCase(name='Validate Schema - Edge Case - Lowest Acceptable Limits', description='Validate a POST request with field values at their lowest acceptable limits to ensure the schema is accepted.', input_json={'field1': '0', 'field2': '0000-01-01'}, expected_output_prompt='Schema validation should succeed with field values at the lowest acceptable limits with no errors.', expected_output_json={'status': 'success', 'message': 'Schema validated successfully.'}, preconditions='The API accepts minimum boundary values for fields.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_5_subtask_2': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate Schema - Stress Case 1000 Requests', description='Send 1000 concurrent POST requests to /pets with valid data to observe system performance and response time under load.', input_json=None, expected_output_prompt='Expected response times should be within acceptable limits (e.g., <200ms per request) without system errors.', expected_output_json=None, preconditions='System is running and is capable of handling multiple requests.'), TestCase(name='Validate Schema - Stress Case Max Size Data', description='Send a POST request to /pets with valid data payload at maximum size allowed to ensure the system can handle large inputs efficiently.', input_json=None, expected_output_prompt='System should accept and process the request without errors, returning success response.', expected_output_json=None, preconditions='Data payload must adhere to the defined schema with maximum size constraints.'), TestCase(name='Validate Schema - Stress Case Response Consistency', description='Send multiple concurrent valid POST requests (e.g., 5000) to /pets to evaluate if the responses maintain consistency with the schema across requests.', input_json=None, expected_output_prompt='Responses should have consistent structure and data type formats across all requests.', expected_output_json=None, preconditions='Schema validation must be enabled on the endpoint.'), TestCase(name='Validate Schema - Stress Case High Frequency Data', description='Submit 100 concurrent POST requests to /pets every second for 10 minutes to test load handling under continuous stress.', input_json=None, expected_output_prompt='System should respond within acceptable times, and should not crash, timeout, or display any errors during the test.', expected_output_json=None, preconditions='System must be monitored for resource utilization during the test.'), TestCase(name='Validate Schema - Stress Case Error Handling for Size', description='Attempt to send a POST request with data larger than the maximum schema size to assess how the system handles overflow and whether proper error messages are returned.', input_json=None, expected_output_prompt='System should return an appropriate error message indicating exceeding maximum size limit with a relevant HTTP status code.', expected_output_json=None, preconditions='Maximum schema size should be defined and enforced on the server.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_5_subtask_3': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate Schema - Missing One Mandatory Field - Normal Case', description=\"Send a POST request with one mandatory field missing (e.g., 'username' is missing) and expect a validation error response indicating the missing field.\", input_json={'username': '', 'email': 'test@example.com', 'password': 'password123'}, expected_output_prompt=\"Response should indicate that the 'username' field is required.\", expected_output_json={'error': \"Validation error: 'username' field is required.\"}, preconditions='API endpoint for user creation is available.'), TestCase(name='Validate Schema - Missing Multiple Mandatory Fields - Normal Case', description=\"Send a POST request with multiple mandatory fields missing (e.g., 'username' and 'email' are missing) and expect a validation error response indicating the missing fields.\", input_json={'username': '', 'email': '', 'password': 'password123'}, expected_output_prompt=\"Response should indicate that the 'username' and 'email' fields are required.\", expected_output_json={'error': \"Validation error: 'username' and 'email' fields are required.\"}, preconditions='API endpoint for user creation is available.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_5_subtask_4': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate Schema - Invalid Data Type - Edge Case: String instead of integer', description='Test POST request where an integer field is sent as a string to validate the service response.', input_json={'field1': 'not_an_integer', 'field2': True}, expected_output_prompt='Expected a 400 Bad Request response indicating invalid data type.', expected_output_json={'error': 'Invalid data type for field1. Expected an integer.'}, preconditions='The service is up and running.'), TestCase(name='Validate Schema - Invalid Data Type - Edge Case: Boolean instead of string', description='Test POST request where a string field is sent as a boolean to validate the service response.', input_json={'field1': False, 'field2': 'not_a_boolean'}, expected_output_prompt='Expected a 400 Bad Request response indicating invalid data type.', expected_output_json={'error': 'Invalid data type for field2. Expected a string.'}, preconditions='The service is up and running.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_5_subtask_5': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate Schema - Business Logic - Age Below Required Limit', description='Send a POST request with an age value below the required limit to verify that the API enforces minimum age constraints effectively.', input_json={'age': 15, 'price': 100.0}, expected_output_prompt='Expected response should indicate a validation error related to age.', expected_output_json={'success': False, 'error': 'Age must be at least 18.'}, preconditions='User must be authenticated and authorized to access the service.'), TestCase(name='Validate Schema - Business Logic - Negative Price Value', description='Send a POST request with a negative price value to check if the API validates price constraints correctly.', input_json={'age': 25, 'price': -50.0}, expected_output_prompt='Expected response should indicate a validation error related to price.', expected_output_json={'success': False, 'error': 'Price must be a positive value.'}, preconditions='User must be authenticated and authorized to access the service.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_5_subtask_6': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate Schema - Excessive Data - Edge Case - Field Length Exceeded', description='Test POST request by exceeding the maximum allowed data length for a specific textual field.', input_json=None, expected_output_prompt='Error: Field exceeds maximum length of 255 characters.', expected_output_json=None, preconditions='Ensure the API endpoint is reachable and ready to accept POST requests.'), TestCase(name='Validate Schema - Excessive Data - Edge Case - Array Length Exceeded', description='Test POST request by submitting an array that exceeds the defined limit of elements.', input_json=None, expected_output_prompt='Error: Array exceeds maximum limit of defined elements.', expected_output_json=None, preconditions='Ensure the API endpoint is reachable and ready to accept POST requests.')], msg=None),\n",
       " 'test_case_family_agent_level_2_task_0_subtask_5_subtask_7': AgentResult(status=<AgentStatus.COMPLETED: 'completed'>, data=[TestCase(name='Validate Schema - API Rate Limiting - Stress Case - Burst Requests', description='Send 200 requests to the API endpoint within 1 second to verify if the rate limiting kicks in and prevents further requests.', input_json=None, expected_output_prompt='HTTP 429 Too Many Requests error or a similar response indicating that the rate limit has been exceeded.', expected_output_json=None, preconditions='The API endpoint should be accessible and the rate limit set should allow for fewer than 200 requests per second.'), TestCase(name='Validate Schema - API Rate Limiting - Stress Case - Sustained High Load', description='Continuously send 100 requests per second for 10 seconds to test if the API maintains its rate limit under sustained high load.', input_json=None, expected_output_prompt=\"HTTP 429 Too Many Requests error after reaching the rate limit threshold as specified in the API's documentation.\", expected_output_json=None, preconditions='The API should have a defined rate limit less than or equal to 100 requests per second.'), TestCase(name='Validate Schema - API Rate Limiting - Stress Case - Cooldown Recovery', description='After sending 500 requests in a short period, wait for the cooldown period and then send more requests to verify if the API restores service appropriately.', input_json=None, expected_output_prompt='Successful response for requests sent after the cooldown period, indicating the rate limit has reset.', expected_output_json=None, preconditions='The API should have a mechanism to recover from rate limiting after a cooldown period.'), TestCase(name='Validate Schema - API Rate Limiting - Stress Case - Combined Techniques', description='Simultaneously trigger a burst of 150 requests followed by sustained 50 requests per second for 10 seconds to test combined scenarios of rate limiting.', input_json=None, expected_output_prompt='HTTP 429 Too Many Requests for some of the requests to verify that rate limiting is enforced during bursts and sustained load.', expected_output_json=None, preconditions='The API should enforce rate limits appropriately according to the specifications.')], msg=None)}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"test_case_generator_agent\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
