{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel, RootModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\"openai:gpt-4o-mini\", name=\"test_case_generator\", retries=1)\n",
    "\n",
    "@agent.system_prompt\n",
    "def test_case_system_prompt():\n",
    "    return \"\"\"You are a helpful test case generator. You will be given a description of functionality \n",
    "    and should generate test cases that thoroughly validate the described behavior.\n",
    "    \n",
    "    <TEST CASE OUTPUT FORMAT>\n",
    "    name: <name of the test case>\n",
    "    description: <description of the test case>\n",
    "    input: <input values for the test case>\n",
    "    expected_output: <expected output/behavior of the test case>\n",
    "    preconditions: <any relevant preconditions for the test case>\n",
    "    </TEST CASE OUTPUT FORMAT>\n",
    "\n",
    "    <OUTPUT FORMAT: list of dictionaries>\n",
    "    [<TEST CASE OUTPUT FORMAT>, <TEST CASE OUTPUT FORMAT>, ...]\n",
    "    </OUTPUT FORMAT>\n",
    "\n",
    "    Be aware of the number of test casaes the user wants and output the correct number of test cases in the correct output format.\n",
    "    \"\"\"\n",
    "\n",
    "agent.result_validator = lambda x: isinstance(x, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_spec = {\n",
    "    \"paths\": {\n",
    "        \"/pets\": {\n",
    "            \"get\": {\n",
    "                \"parameters\": [\n",
    "                    {\n",
    "                        \"name\": \"status\",\n",
    "                        \"in\": \"query\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"required\": True,\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"post\": {\n",
    "                \"requestBody\": {\n",
    "                    \"content\": {\n",
    "                        \"application/json\": {\n",
    "                            \"schema\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"name\": {\"type\": \"string\"},\n",
    "                                    \"age\": {\"type\": \"integer\"},\n",
    "                                },\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent.run(\"Generate one case for this API spec: \" + str(api_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'description': 'Test retrieving a list of pets with a valid status',\n",
      "  'expected_output_json': None,\n",
      "  'expected_output_prompt': None,\n",
      "  'input_json': None,\n",
      "  'name': 'GetPetsByStatus',\n",
      "  'preconditions': 'The API server is running and has pets with status '\n",
      "                   \"'available' in the database.\"}]\n"
     ]
    }
   ],
   "source": [
    "class TestCase(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_json: dict[str, Any] | list[dict[str, Any]] | None = None\n",
    "    expected_output_prompt: str | None = None\n",
    "    expected_output_json: dict[str, Any] | list[dict[str, Any]] | None = None\n",
    "    preconditions: str | None = None\n",
    "\n",
    "class SuiteTestCases(RootModel[list[TestCase]]):\n",
    "    pass\n",
    "\n",
    "test_cases = SuiteTestCases.model_validate_json(response.data)\n",
    "\n",
    "pprint(test_cases.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_agent = Agent(\"openai:gpt-4o-mini\", name=\"test_case_generator\", retries=1)\n",
    "\n",
    "@agent.system_prompt\n",
    "def test_case_system_prompt():\n",
    "    return \"\"\"You are a helpful test case generator. You will be given a description of functionality \n",
    "    and should generate test cases that thoroughly validate the described behavior.\n",
    "    \n",
    "    <TEST CASE OUTPUT FORMAT>\n",
    "    name: <name of the test case>\n",
    "    description: <description of the test case>\n",
    "    input: <input values for the test case>\n",
    "    expected_output: <expected output/behavior of the test case>\n",
    "    preconditions: <any relevant preconditions for the test case>\n",
    "    </TEST CASE OUTPUT FORMAT>\n",
    "\n",
    "    <OUTPUT FORMAT: list of dictionaries>\n",
    "    [<TEST CASE OUTPUT FORMAT>, <TEST CASE OUTPUT FORMAT>, ...]\n",
    "    </OUTPUT FORMAT>\n",
    "\n",
    "    Be aware of the number of test casaes the user wants and output the correct number of test cases in the correct output format.\n",
    "    \"\"\"\n",
    "\n",
    "agent.result_validator = lambda x: isinstance(x, list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
